import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from torch.utils.data import Dataset, DataLoader, random_split
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import pymysql, os
from sqlalchemy import create_engine
from dotenv import load_dotenv

import seaborn as sns
import matplotlib.pyplot as plt
import wandb

# --- í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° ---
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "../../..", ".env"))

# ======================
# 1. MySQL ì—°ê²°
# ======================
# ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”
USER = os.getenv("MYSQL_USER")
PASSWORD = os.getenv("MYSQL_PASSWORD")
HOST = "localhost"
PORT = 3306
DB = os.getenv("MYSQL_DATABASE")

db_con = f"mysql+pymysql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}"
# print(db_con)

# SQLAlchemy ì—”ì§„ ìƒì„±
engine = create_engine(db_con)

# ======================
# 2. í…Œì´ë¸” ë¶ˆëŸ¬ì˜¤ê¸°
# ======================
item_retail = pd.read_sql("SELECT * FROM item_retail", engine)
sea_weather = pd.read_sql("SELECT * FROM sea_weather", engine)
ground_weather = pd.read_sql("SELECT * FROM ground_weather", engine)
location = pd.read_sql("SELECT * FROM location", engine)
item = pd.read_sql("SELECT * FROM item", engine)

print("item_retail sample:")
print(item_retail.head())

# ======================
# 3. í…Œì´ë¸” ë¨¸ì§€ (JOIN)
# ======================
# sea_weather wide-format ë³€í™˜
sea_weather = sea_weather.merge(location, on="local_pk", how="left")
sea_weather_wide = sea_weather.pivot(
    index="month_date", 
    columns="local_pk", 
    values=["temperature", "wind", "salinity", "wave_height", "wave_period", "wave_speed", "rain", "snow"]
)

# ì»¬ëŸ¼ëª… ì •ë¦¬
sea_weather_wide.columns = [f"{var}_loc{loc}" for var, loc in sea_weather_wide.columns]
sea_weather_wide = sea_weather_wide.reset_index()
print(sea_weather_wide)

df = item_retail.merge(sea_weather_wide, on="month_date", how="left")
df = df.merge(ground_weather, on="month_date", how="left")
df = df.merge(item, on="item_pk", how="left")

# ë‚ ì§œ ì •ë ¬
df["month_date"] = pd.to_datetime(df["month_date"])
df = df.sort_values(["month_date"]).reset_index(drop=True)
df["month_num"] = df["month_date"].dt.year * 12 + df["month_date"].dt.month
df = pd.get_dummies(df, columns=['item_name'])

print("Merged DataFrame:")
print(df.head())
df.to_csv("compare_production.csv", index=False, encoding="utf-8-sig")

# ======================
# 4. ì‹œê³„ì—´ ìœˆë„ìš° ë°ì´í„°ì…‹ ìƒì„±
# ======================
class TimeSeriesDataset(Dataset):
    def __init__(self, df, feature_cols, target_cols, window_size=6):
        self.window_size = window_size
        self.features = df[feature_cols].values
        self.targets = df[target_cols].values

        # í‘œì¤€í™”
        self.scaler_x = StandardScaler()
        self.scaler_y = StandardScaler()

        self.features = self.scaler_x.fit_transform(self.features)
        self.targets = self.scaler_y.fit_transform(self.targets)

    def __len__(self):
        return len(self.features) - self.window_size

    def __getitem__(self, idx):
        X = self.features[idx:idx+self.window_size]
        y = self.targets[idx+self.window_size]
        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# ======================
# 5. PyTorch ëª¨ë¸ ì •ì˜
# ======================
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, output_dim=1, num_layers=2):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        out = self.fc(h_n[-1])  # ë§ˆì§€ë§‰ hidden state
        return out

class SimpleRNNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, output_dim=1, num_layers=2):
        super().__init__()
        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)
        
        # ì¤‘ê°„ hidden layer ì¶”ê°€
        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim // 2)
        self.relu = nn.ReLU()

        self.fc = nn.Linear(hidden_dim // 2, output_dim)

    def forward(self, x):
        _, h_n = self.rnn(x)
       
        # ì¶”ê°€ hidden layer í†µê³¼
        h_n = self.hidden_layer(h_n[-1])     # (batch, hidden_dim//2)
        h_n = self.relu(h_n)

        out = self.fc(h_n)
        return out

class GRUModel(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, output_dim=1, num_layers=2):
        super().__init__()
        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        _, h_n = self.gru(x)
        out = self.fc(h_n[-1])
        return out

# feature embedding â†’ Transformer Encoder â†’ FC regression head
class TransformerEncoderModel(nn.Module):
    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, output_dim=1):
        super().__init__()
        self.input_fc = nn.Linear(input_dim, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(d_model, output_dim)

    def forward(self, x):
        x = self.input_fc(x)
        x = self.transformer(x)
        # ë§ˆì§€ë§‰ ì‹œì  ì„ íƒ
        out = self.fc(x[:, -1, :])
        return out
    
# ======================
# 6. í•™ìŠµ ë£¨í”„
# ======================
def train_and_evaluate(model, train_loader, val_loader, epochs=40, lr=1e-3, model_name="model.pth"):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    best_rmse = float("inf")  # ì•„ì£¼ í° ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
    best_mae = float("inf")  # ì•„ì£¼ í° ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
    best_r2 = float("inf")  # ì•„ì£¼ í° ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
    best_state = None

    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for X, y in train_loader:
            optimizer.zero_grad()
            preds = model(X)
            loss = criterion(preds, y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # ê²€ì¦
        model.eval()
        val_loss = 0
        y_true, y_pred = [], []
        with torch.no_grad():
            for X, y in val_loader:
                preds = model(X)
                val_loss += criterion(preds, y).item()
                y_true.extend(y.numpy())
                y_pred.extend(preds.numpy())

        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)

        print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss/len(train_loader):.4f} | "
              f"Val Loss: {val_loss/len(val_loader):.4f} | RMSE: {rmse:.2f} | MAE: {mae:.2f} | RÂ²: {r2:.2f}")
        
        # ğŸš€ wandbì— ë¡œê·¸ ê¸°ë¡
        wandb.log({
            "epoch": epoch+1,
            "train_loss": avg_train_loss,
            "val_loss": avg_val_loss,
            "rmse": rmse,
            "mae": mae,
            "r2": r2,
            "model": model_name
        })

        # âœ… ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì €ì¥
        if rmse < best_rmse:
            best_rmse = rmse
            best_mae = mae
            best_r2 = r2
            best_state = model.state_dict()
            torch.save(best_state, f"{model_name}_sales.pth")
            print(f"  ğŸ‘‰ Best model saved (epoch {epoch+1}, RMSE={rmse:.2f})")


    # ìµœì¢… ì„±ëŠ¥ ë¦¬í„´
    return best_rmse, best_mae, best_r2

# ======================
# 7. ì‹¤í–‰
# ======================

# # ì‚¬ìš©í•  ì»¬ëŸ¼ ì •ì˜ (ì˜ˆì‹œ)
target_cols = ["production"]
feature_cols = [x for x in df.columns if x not in ["month_date", "production", "sales", "ground_pk", "item_pk", "retail_pk", "item_pk", "local_pk", "sea_pk"]]

from sklearn.decomposition import PCA
X = df[feature_cols].values  # sklearnì€ numpy ì…ë ¥

# í‘œì¤€í™” (TimeSeriesDatasetì—ì„œë„ StandardScaler í–ˆì§€ë§Œ PCAìš© ë³„ë„)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print("X_scaledì˜ shape:", X_scaled.shape)

# PCA ì ìš©
pca = PCA(n_components=21)  # ì›í•˜ëŠ” ì£¼ì„±ë¶„ ê°œìˆ˜
X_pca = pca.fit_transform(X_scaled)

# shape í™•ì¸
print(X_pca.shape)  # (num_samples, 20)

# # Dataset ì¤€ë¹„
# dataset = TimeSeriesDataset(df, feature_cols, target_cols, window_size=6)
dataset = TimeSeriesDataset(df, feature_cols, target_cols, window_size=6)

# Train / Validation Split
train_size = int(len(dataset) * 0.8)
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# ëª¨ë¸ ì´ˆê¸°í™”
input_dim = len(feature_cols)

# í•™ìŠµ
models = {
    "LSTM": LSTMModel(input_dim=len(feature_cols), hidden_dim=64, output_dim=len(target_cols)),
    "SimpleRNN": SimpleRNNModel(input_dim=len(feature_cols), hidden_dim=64, output_dim=len(target_cols)),
    "GRU": GRUModel(input_dim=len(feature_cols), hidden_dim=64, output_dim=len(target_cols)),
    "Transformer": TransformerEncoderModel(input_dim=len(feature_cols), d_model=64, nhead=4, num_layers=2, output_dim=len(target_cols))
}

results = {}

for name, model in models.items():
    print(f"\n===== Training {name} =====")
    # í”„ë¡œì íŠ¸ëª…, ì—”í‹°í‹°(ê³„ì •ëª… ë˜ëŠ” íŒ€ëª…), í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡
    wandb.init(
        project="DataTide_production_compare_model",   # ì›í•˜ëŠ” í”„ë¡œì íŠ¸ ì´ë¦„
        entity=os.getenv("WANDB_ENTITY"),       # ë³¸ì¸ ê³„ì •ëª…
        config={
            "epochs": 100,
            "learning_rate": 1e-3,
            "batch_size": 32,
            "window_size": 6,
            "hidden_dim": 64,
            "model":name
        },
        name=name,
        reinit=True   # run ìƒˆë¡œ ì‹œì‘
    )
    rmse, mae, r2 = train_and_evaluate(model, train_loader, val_loader, 
                                       epochs=wandb.config.epochs, 
                                       lr=wandb.config.learning_rate, 
                                       model_name=name)
    results[name] = {"RMSE": rmse, "MAE": mae, "R2": r2}

print("\n===== Model Comparison =====")
for name, metric in results.items():
    print(f"{name}: RMSE={metric['RMSE']:.2f}, MAE={metric['MAE']:.2f}, RÂ²={metric['R2']:.2f}")

def drawHitmap():
# íˆíŠ¸ë§µ.
correlation_matrix = df[feature_cols + target_cols].corr()     # ë°ì´í„° í”„ë ˆì„ì´ corr ì´ë¼ëŠ” í•¨ìˆ˜ê°€ ìˆì–´ì„œ ìƒê´€ê³„ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.
print(correlation_matrix[:10])

# 2. íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
annot = False    # ì°¨íŠ¸ì— ì¤„ ì†ì„±. íˆíŠ¸ë§µì˜ ì…€ì— ê°’ì„ í‘œì‹œí•œë‹¤. Falseë©´ í‘œì‹œ ì•ˆ í•¨.
cmap = 'coolwarm'   # íˆíŠ¸ë§µì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ìƒ‰ìƒ. ì–‘ì˜ê´€ê³„ëŠ” ë¹¨ê°„ìƒ‰, ìŒì˜ê´€ê³„ëŠ” íŒŒë€ìƒ‰
fmt = '.2f'     # í‘œì‹œë  ìˆ«ìì˜ ì†Œìˆ˜ì  ìë¦¬ìˆ˜ ì§€ì •
sns.heatmap(correlation_matrix,
            annot=annot, cmap=cmap, fmt=fmt, 
            linewidths=.5)      # ì…€ ì‚¬ì´ì— ì„  ì¶”ê°€
plt.xticks(rotation=45, ha='right')     #  xì¶• ë ˆì´ë¸” íšŒì „
plt.yticks(rotation=0)
plt.tight_layout()      # ë ˆì´ë¸” ê²¹ì¹¨ ë°©ì§€. ë‹¤ì‹œ ê·¸ë ¤ë¼
plt.show()

