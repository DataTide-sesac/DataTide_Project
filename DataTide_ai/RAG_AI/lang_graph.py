import mysql.connector
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# ì±„íŒ…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì–‘ì‹ì„ ë§Œë“¦
from langchain.prompts import ChatPromptTemplate
from langchain.agents import AgentExecutor, create_tool_calling_agent
import langchain

# ì‹œê°„ ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime, timedelta
import os, pickle, time, hashlib
import numpy as np
import json, requests
from typing import Any, Dict, Optional
from tempfile import tempdir
from dotenv import load_dotenv

from langgraph.graph import StateGraph, START, END  # ìƒíƒœê·¸ë˜í”„ì™€ ENDë…¸ë“œ ì„¸íŒ…
from typing_extensions import TypedDict             # ìƒíƒœê·¸ë˜í”„ ì»¤ìŠ¤í„°ë§ˆì´ì§•
from langchain.agents import Tool
from langchain.chains import LLMMathChain

# tool ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ì¼ë°˜ í•¨ìˆ˜ë¥¼ 'íˆ´'ë¡œ ì¸ì§€ì‹œí‚¤ê¸° ìœ„í•¨
from langchain.tools import tool
#ë§Œë“  íˆ´ì„ ì‚¬ìš©í•˜ì!
from langchain.chat_models import init_chat_model


# --- í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° ---
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "../..", ".env"))
openai_api_key = os.getenv("OPENAI_API_KEY3")
openai_api_key2 = os.getenv("OPENAI_API_KEY2")

mysql_user = os.getenv("MYSQL_USER")
mysql_password = os.getenv("MYSQL_PASSWORD")
mysql_database = os.getenv("MYSQL_DATABASE")

# --- 1. MySQL ì—°ê²° ---
conn = mysql.connector.connect(
    host="localhost",
    user=mysql_user,
    password=mysql_password,
    database=mysql_database
)

# ë””ë¹„ì—ì„œ ì½ê¸°
cursor = conn.cursor()
texts = []
def read_sql_1():
    sql = """
        SELECT 
            CASE 
                WHEN i.item_name = 'Calamari' THEN 'ì˜¤ì§•ì–´'
                WHEN i.item_name = 'CutlassFish' THEN 'ê°ˆì¹˜'
                WHEN i.item_name = 'Mackerel' THEN 'ê³ ë“±ì–´'
                ELSE i.item_name
            END AS item_name,
        ir.month_date, ir.production, ir.inbound, ir.sales, 
        gw.temperature as gw_temp, gw.rain as gw_rain,
        l.local_name,
        sw.temperature as sw_temp, sw.wind, sw.salinity, sw.wave_height, sw.wave_period, sw.wave_speed, sw.rain as sw_rain, sw.snow as sw_snow
        FROM item_retail ir
        LEFT JOIN sea_weather sw ON ir.month_date = sw.month_date
        LEFT JOIN ground_weather gw ON ir.month_date = gw.month_date
        LEFT JOIN location l ON sw.local_pk = l.local_pk
        LEFT JOIN item i ON ir.item_pk = i.item_pk
    """
    cursor.execute(sql)
    rows = cursor.fetchall()

    texts = [f"""í’ˆëª©: {row[0]}, ë‚ ì§œ: {row[1]}, ìƒì‚°ëŸ‰: {row[2]}, ìˆ˜ì…ëŸ‰: {row[3]}, íŒë§¤ëŸ‰: {row[4]},
            ì „êµ­ ì§€ìƒ ë‚ ì”¨: {row[5]}, ì „êµ­ ì§€ìƒ ê°•ìˆ˜ëŸ‰: {row[6]}, ì§€ì—­: {row[7]},
            í•´ì–‘ ë‚ ì”¨: {row[8]}, ì—¼ë¶„: {row[9]}, ìœ ì˜íŒŒê³ : {row[10]}, ìœ ì˜íŒŒì£¼ê¸°: {row[11]}, ìœ ì†: {row[12]}, í•´ì–‘ ê°•ìˆ˜ëŸ‰: {row[13]}, í•´ì–‘ ì ì„¤ëŸ‰: {row[14]}
            """
            for row in rows]
    
def read_sql_2():
    sql = """
        SELECT 
            CASE 
                WHEN i.item_name = 'Calamari' THEN 'ì˜¤ì§•ì–´'
                WHEN i.item_name = 'CutlassFish' THEN 'ê°ˆì¹˜'
                WHEN i.item_name = 'Mackerel' THEN 'ê³ ë“±ì–´'
                ELSE i.item_name
            END AS item_name,
        ir.month_date, ir.production, ir.inbound, ir.sales
        FROM item_retail ir
        LEFT JOIN item i ON ir.item_pk = i.item_pk
    """
    cursor.execute(sql)
    rows = cursor.fetchall()

    texts = [f"""í’ˆëª©: {row[0]}, ë‚ ì§œ: {row[1]}, ìƒì‚°ëŸ‰: {row[2]}, ìˆ˜ì…ëŸ‰: {row[3]}, íŒë§¤ëŸ‰: {row[4]}
            """
            for row in rows]
    
    return texts
    
texts = read_sql_2()

# --- 2. ì²­í¬í™” ---
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)
chunks = []
for text in texts:
    chunks.extend(splitter.split_text(text))

# --- 3. ì„ë² ë”© & ë²¡í„° DB ---
embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-m3')

if os.path.exists("faiss_index"):
    vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
else:
    vectorstore = FAISS.from_texts(chunks, embeddings)
    vectorstore.save_local("faiss_index")

print("ì„ë² ë”© & FAISS DB ì¤€ë¹„ ì™„ë£Œ!")

# --- 4. LLM ì¤€ë¹„ ---
model = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0,
    openai_api_key=openai_api_key,
    max_tokens=512  # ë‹µë³€ ê¸¸ì´ë¥¼ ëŠ˜ë ¤ì¤ë‹ˆë‹¤.
)

# ë™ì¼í•œ ì„¸íŒ…ì˜ llmì„ LLMMathChainì—ê²Œ ì „ë‹¬
llm_math = LLMMathChain(llm=ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
        openai_api_key=openai_api_key,
        max_tokens=256  # ë‹µë³€ ê¸¸ì´ë¥¼ ëŠ˜ë ¤ì¤ë‹ˆë‹¤.
    ))

# ìˆ˜í•™ì  íˆ´
math_tool = Tool(
    name = 'calculator',
    func = llm_math.run,
    description = 'ìˆ˜í•™ ê³„ì‚°ê³¼ ê´€ë ¨ëœ ë¬¸ì œì— ëŒ€í•´ í•„ìš”í•˜ë©´ ì´ ë„êµ¬ë¥¼ ì“°ì‹œì˜¤.'
)

# --- 5. ì¶”ê°€ ë°ì´í„° ë¶„ì„ ë„êµ¬ë“¤ ---
@tool
def analyze_data(query: str) -> str:
    """RAG ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ìµœëŒ€ê°’, ìµœì†Œê°’, í‰ê· , í•©ê³„ ë“±ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    # ğŸ”¥ ìºì‹œ í™•ì¸ (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
    cached_result = cache.get(query, "analyze")
    if cached_result:
        return f"[ìºì‹œë¨] {cached_result}"  # ğŸš€ ì¦‰ì‹œ ë¦¬í„´!
    
    try:
        import re
        import statistics
        from collections import defaultdict
        
        # ë²¡í„° ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        docs = vectorstore.similarity_search(query, k=100)
        
        # ìˆ«ì ë°ì´í„° ì¶”ì¶œ
        data_dict = defaultdict(list)
        
        for doc in docs:
            content = doc.page_content
            
            # ìƒì‚°ëŸ‰, ìˆ˜ì…ëŸ‰, íŒë§¤ëŸ‰ ë“±ì˜ ìˆ«ì ë°ì´í„° ì¶”ì¶œ
            patterns = {
                'ìƒì‚°ëŸ‰': r'ìƒì‚°ëŸ‰:\s*([0-9.]+)',
                'ìˆ˜ì…ëŸ‰': r'ìˆ˜ì…ëŸ‰:\s*([0-9.]+)',
                'íŒë§¤ëŸ‰': r'íŒë§¤ëŸ‰:\s*([0-9.]+)'
            }
            
            for key, pattern in patterns.items():
                matches = re.findall(pattern, content)
                for match in matches:
                    try:
                        value = float(match)
                        if value > 0:  # 0ë³´ë‹¤ í° ê°’ë§Œ í¬í•¨
                            data_dict[key].append(value)
                    except ValueError:
                        continue
        
        # ë¶„ì„ ê²°ê³¼ ìƒì„±
        result = "=== ë°ì´í„° ë¶„ì„ ê²°ê³¼ ===\n"
        
        for key, values in data_dict.items():
            if values:
                result += f"\nã€{key}ã€‘\n"
                result += f"  ë°ì´í„° ê°œìˆ˜: {len(values)}ê°œ\n"
                result += f"  ìµœëŒ€ê°’: {max(values):.2f}\n"
                result += f"  ìµœì†Œê°’: {min(values):.2f}\n"
                result += f"  í‰ê· ê°’: {statistics.mean(values):.2f}\n"
                result += f"  í•©ê³„: {sum(values):.2f}\n"
                
                if len(values) > 1:
                    result += f"  í‘œì¤€í¸ì°¨: {statistics.stdev(values):.2f}\n"
        
        # âœ… ìºì‹œ ì €ì¥ ì¶”ê°€
        final_result = result if result != "=== ë°ì´í„° ë¶„ì„ ê²°ê³¼ ===\n" else "ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìˆ«ì ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        cache.set(query, {"result": final_result}, "analyze")
        
        return result if result != "=== ë°ì´í„° ë¶„ì„ ê²°ê³¼ ===\n" else "ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìˆ«ì ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    except Exception as e:
        return f"ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

tools = [math_tool]
# tools = [math_tool, analyze_data]
print(tools[0].name, tools[0].description)

# model(ìƒê°í•  llm, agentì˜ ë‘ë‡Œ),
# tools(agentê°€ ì‚¬ìš©ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡),
# prompt(agentê°€ íŒë‹¨í•  ë•Œ ì–´ë–¤ ê¸°ì¤€ì´ ìˆë‹¤ë©´? ì´ê±¸ í”„ë¡¬í”„íŠ¸ê°€ ë‹´ê³  ìˆë‹¤.)
prompt = ChatPromptTemplate.from_messages([
    ('system', '''ë‹¹ì‹ ì€ í’ˆëª©ë‹¹(ê°ˆì¹˜, ì˜¤ì§•ì–´, ê³ ë“±ì–´) ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ë§Œì•½ í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´ ì €í¬ëŠ” í’ˆëª©ë‹¹ ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ë§Œ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì´ë¼ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì¤˜.
    ëŒ€ë‹µì€ ì¹œì ˆí•˜ê²Œ í•´ì£¼ì„¸ìš”.
    ë‹¤ìŒ ë„êµ¬ë“¤ì„ í™œìš©í•  ìˆ˜ ìˆì–´:
    1. calculator: ìˆ˜í•™ ê³„ì‚°
    2. analyze_data: ë°ì´í„° ë¶„ì„ ë° í†µê³„ ê³„ì‚°
    
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•´ì„œ ì‚¬ìš©í•´ì¤˜.'''),
    ('human', '{input}'),
    ('placeholder', '{agent_scratchpad}')   # toolsì„ ì‚¬ìš©í•˜ë©´ì„œ ë‚¨ê¸°ëŠ” ì¤‘ê°„ ì‘ì—… ë‚´ì—­
])

# agentë¥¼ ë§Œë“¦ => ì´ agentëŠ” toolì„ calling =>
# model(ë‘ë‡Œ)ë¡œ tools(ë„êµ¬)ë¥¼ prompt(ì§€ì‹œì‚¬í•­)ì— ë§ê²Œ íŒë‹¨í•´ì„œ ë¶€ë¦„
agent = create_tool_calling_agent(model, tools, prompt)

# ì—ì´ì „íŠ¸ì˜ ì‹¤í–‰
agent_executor = AgentExecutor(agent=agent, tools=tools)
# result = agent_executor.invoke({'input':'what is 2.1^2.1'})

# --- 6. RetrievalQA ì²´ì¸ êµ¬ì„± ---
qa_chain = RetrievalQA.from_chain_type(
    llm=model,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 40}),
    return_source_documents=True,
)

# --- 7. LangGraph State ì •ì˜ ---
class AgentState(TypedDict):
    query: str
    chat_history: list
    current_time: str
    rag_result: str
    agent_result: str
    final_answer: str
    needs_agent: bool
    source_documents: list

# ==========================================
# í•´ê²°ì±… 1: ì§ˆë¬¸ ì •ê·œí™” ë° ì˜ë¯¸ ê¸°ë°˜ ìºì‹±
# ==========================================

class SmartCache:
    def __init__(self, cache_dir="cache", expire_hours=12, use_json=True):
        self.cache_dir = cache_dir
        self.expire_hours = expire_hours
        self.use_json = use_json  # JSON vs Pickle ì„ íƒ
        self.memory_cache = {}

        self.access_count = 0
        self.last_cleanup = datetime.now()
        self.running = False
        self.cleanup_thread = None
        
        # ğŸ”¥ ì˜ë¯¸ ê¸°ë°˜ ìºì‹±ì„ ìœ„í•œ ì„ë² ë”© ëª¨ë¸
        self.embeddings = embeddings

        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)

        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        self.metadata_file = os.path.join(cache_dir, "cache_metadata.json")
        # ê¸°ì¡´ ìºì‹œ ë¡œë“œ
        self._load_existing_caches()
        
        print(f"íŒŒì¼ ê¸°ë°˜ ìºì‹œ ì´ˆê¸°í™”: {cache_dir}")
        print(f"ì €ì¥ í˜•ì‹: {'JSON' if use_json else 'Pickle'}")
        print(f"ê¸°ì¡´ ìºì‹œ: {len(self.memory_cache)}ê°œ")

        print(f"ğŸ—„ï¸ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì´ˆê¸°í™”: {cache_dir}, ë§Œë£Œ: {expire_hours}ì‹œê°„")
        
        # ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        self._start_background_cleanup()

    def _start_background_cleanup(self):
        """ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘"""
        import threading
        
        self.running = True
        self.cleanup_thread = threading.Thread(target=self._background_worker, daemon=True)
        self.cleanup_thread.start()
        print("ğŸ§µ ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘")
    
    def _get_cache_key(self, query: str, context_type: str = "rag") -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ì¿¼ë¦¬ë¥¼ í•´ì‹œí™”í•´ì„œ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©
        query_hash = hashlib.md5(query.encode()).hexdigest()
        return f"{context_type}_{query_hash}"
    
    def _get_file_path(self, cache_key: str) -> str:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        extension = ".json" if self.use_json else ".pkl"
        return os.path.join(self.cache_dir, f"{cache_key}{extension}")
    
    def _save_to_file(self, cache_key: str, cached_data: Dict) -> bool:
        """íŒŒì¼ì— ìºì‹œ ë°ì´í„° ì €ì¥"""
        file_path = self._get_file_path(cache_key)
        
        try:
            if self.use_json:
                # JSON ì €ì¥ (ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆìŒ)
                json_data = {
                    'result': cached_data['result'],
                    'timestamp': cached_data['timestamp'].isoformat(),
                    'original_query': cached_data['original_query'],
                    'context_type': cached_data['context_type']
                }
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)
            else:
                # Pickle ì €ì¥ (ë” ë¹ ë¥´ê³  ëª¨ë“  ê°ì²´ ì§€ì›)
                with open(file_path, 'wb') as f:
                    pickle.dump(cached_data, f)
            
            print(f"íŒŒì¼ ì €ì¥ ì™„ë£Œ: {cache_key}")
            return True
            
        except Exception as e:
            print(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜ ({cache_key}): {e}")
            return False
    
    def _load_from_file(self, cache_key: str) -> Optional[Dict]:
        """íŒŒì¼ì—ì„œ ìºì‹œ ë°ì´í„° ë¡œë“œ"""
        file_path = self._get_file_path(cache_key)
        
        if not os.path.exists(file_path):
            return None
        
        try:
            if self.use_json:
                with open(file_path, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                
                # JSONì—ì„œ datetime ë³µì›
                cached_data = {
                    'result': json_data['result'],
                    'timestamp': datetime.fromisoformat(json_data['timestamp']),
                    'original_query': json_data['original_query'],
                    'context_type': json_data['context_type']
                }
            else:
                with open(file_path, 'rb') as f:
                    cached_data = pickle.load(f)
            
            return cached_data
            
        except Exception as e:
            print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({cache_key}): {e}")
            # ì†ìƒëœ íŒŒì¼ ì‚­ì œ
            try:
                os.remove(file_path)
            except:
                pass
            return None

    def _load_existing_caches(self):
        """ê¸°ì¡´ì— ì €ì¥ëœ ëª¨ë“  ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        if not os.path.exists(self.cache_dir):
            return
        
        loaded_count = 0
        extension = ".json" if self.use_json else ".pkl"
        
        for filename in os.listdir(self.cache_dir):
            if filename.endswith(extension) and filename != "cache_metadata.json":
                cache_key = filename.replace(extension, "")
                cached_data = self._load_from_file(cache_key)
                
                if cached_data and not self._is_expired(cached_data['timestamp']):
                    self.memory_cache[cache_key] = cached_data
                    loaded_count += 1
                elif cached_data:  # ë§Œë£Œëœ ê²½ìš°
                    self._delete_file(cache_key)
        
        print(f"ê¸°ì¡´ ìºì‹œ ë¡œë“œ: {loaded_count}ê°œ")

    def _delete_file(self, cache_key: str):
        """ìºì‹œ íŒŒì¼ ì‚­ì œ"""
        file_path = self._get_file_path(cache_key)
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                print(f"íŒŒì¼ ì‚­ì œ: {cache_key}")
        except Exception as e:
            print(f"íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜ ({cache_key}): {e}")
    
    def _background_worker(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ ì •ë¦¬"""
        while self.running:
            try:
                # 30ë¶„ë§ˆë‹¤ ì •ë¦¬
                time.sleep(1800)
                
                if not self.running:
                    break
                
                start_time = time.time()
                cleaned = self._cleanup_expired_caches()
                elapsed = time.time() - start_time
                
                if cleaned > 0:
                    print(f"ğŸ§¹ ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬: {cleaned}ê°œ ì‚­ì œ ({elapsed:.2f}ì´ˆ)")
                
            except Exception as e:
                print(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì˜¤ë¥˜: {e}")
                time.sleep(300)  # ì˜¤ë¥˜ì‹œ 5ë¶„ ëŒ€ê¸°
    
    def _cleanup_expired_caches(self):
        """ë§Œë£Œëœ ìºì‹œë“¤ì„ ì‹¤ì œë¡œ ì •ë¦¬"""
        cleaned_count = 0
        current_time = datetime.now()
        expire_threshold = timedelta(hours=self.expire_hours)
        
        # ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
        expired_keys = []
        for key, cached_data in self.memory_cache.items():
            if current_time - cached_data['timestamp'] > expire_threshold:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.memory_cache[key]
            cleaned_count += 1
        
        # ë””ìŠ¤í¬ ìºì‹œ ì •ë¦¬  
        if os.path.exists(self.cache_dir):
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    file_path = os.path.join(self.cache_dir, filename)
                    try:
                        with open(file_path, 'rb') as f:
                            cached_data = pickle.load(f)
                        
                        if current_time - cached_data['timestamp'] > expire_threshold:
                            os.remove(file_path)
                            cleaned_count += 1
                    except:
                        # ì†ìƒëœ íŒŒì¼ë„ ì‚­ì œ
                        try:
                            os.remove(file_path)
                            cleaned_count += 1
                        except:
                            pass

        # ê³ ì•„ íŒŒì¼ë“¤ ì •ë¦¬ (ë©”ëª¨ë¦¬ì—ëŠ” ì—†ì§€ë§Œ ë””ìŠ¤í¬ì— ìˆëŠ” íŒŒì¼)
        extension = ".json" if self.use_json else ".pkl"
        if os.path.exists(self.cache_dir):
            for filename in os.listdir(self.cache_dir):
                if filename.endswith(extension) and filename != "cache_metadata.json":
                    cache_key = filename.replace(extension, "")
                    if cache_key not in self.memory_cache:
                        cached_data = self._load_from_file(cache_key)
                        if cached_data and self._is_expired(cached_data['timestamp']):
                            self._delete_file(cache_key)
                            cleaned_count += 1
        
        return cleaned_count
    
    def export_cache_summary(self, output_file: str = None):
        """ìºì‹œ ìš”ì•½ ì •ë³´ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if output_file is None:
            output_file = os.path.join(self.cache_dir, "cache_summary.json")
        
        summary = {
            'export_time': datetime.now().isoformat(),
            'total_caches': len(self.memory_cache),
            'cache_stats': self.get_stats(),
            'cache_list': []
        }
        
        # ê° ìºì‹œ ì •ë³´
        for key, cached_data in self.memory_cache.items():
            cache_info = {
                'key': key,
                'query': cached_data['original_query'],
                'context_type': cached_data['context_type'],
                'timestamp': cached_data['timestamp'].isoformat(),
                'age_hours': (datetime.now() - cached_data['timestamp']).total_seconds() / 3600
            }
            summary['cache_list'].append(cache_info)
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            print(f"ìºì‹œ ìš”ì•½ ì €ì¥: {output_file}")
        except Exception as e:
            print(f"ìºì‹œ ìš”ì•½ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def stop_cleanup(self):
        """ìºì‹œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì¤‘ì§€"""
        self.running = False
        if self.cleanup_thread and self.cleanup_thread.is_alive():
            self.cleanup_thread.join(timeout=5)
        print("ğŸ›‘ ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì •ë¦¬ ì¤‘ì§€")
    
    def __del__(self):
        """ìºì‹œ ê°ì²´ ì†Œë©¸ë¨"""
        self.stop_cleanup()
    
    def clear_all(self):
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        # ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ
        cache_keys = list(self.memory_cache.keys())
        for key in cache_keys:
            del self.memory_cache[key]
            self._delete_file(key)
        
        print(f"ëª¨ë“  ìºì‹œ ì‚­ì œ ì™„ë£Œ: {len(cache_keys)}ê°œ")
        
    def _normalize_query(self, query):
        """ì§ˆë¬¸ì„ ì •ê·œí™”í•´ì„œ ìœ ì‚¬í•œ ì§ˆë¬¸ë“¤ì„ ê°™ê²Œ ë§Œë“¦"""        
        # 1. ê¸°ë³¸ ì •ê·œí™”
        normalized = query.lower().strip()
        
        # 2. ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        stop_words = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì„œ', 'ì—ì„œ', 
                     'ì–´ë–»ê²Œ', 'ì–¼ë§ˆ', 'ë­', 'ë¬´ì—‡', 'ì–¸ì œ', 'ì–´ë””ì„œ']
        for word in stop_words:
            normalized = normalized.replace(word, '')
        
        # 3. ê³µë°± ì •ë¦¬
        normalized = ' '.join(normalized.split())
        
        # 4. ë™ì˜ì–´ í†µì¼
        synonyms = {
            'ìƒì‚°ëŸ‰': ['ìƒì‚°', 'ìƒì‚°ëŸ‰', 'ì‚°ì¶œëŸ‰', 'production'],
            'íŒë§¤ëŸ‰': ['íŒë§¤', 'íŒë§¤ëŸ‰', 'ë§¤ì¶œëŸ‰', 'sales'],
            'ìˆ˜ì…ëŸ‰': ['ìˆ˜ì…', 'ìˆ˜ì…ëŸ‰', 'import', 'inbound'],
            'ê°ˆì¹˜': ['ê°ˆì¹˜', 'galchi', 'ê°ˆì¹˜ìƒì„ ', 'cutlassFish'],
            'ì˜¤ì§•ì–´': ['ì˜¤ì§•ì–´', 'squid', 'ì˜¤ì§•ì–´ë¥˜', 'calamari'],
            'ê³ ë“±ì–´': ['ê³ ë“±ì–´', 'mackerel', 'ê³ ë“±ì–´ë¥˜']
        }
        
        for standard, variations in synonyms.items():
            for variation in variations:
                if variation in normalized:
                    normalized = normalized.replace(variation, standard)
        
        return normalized
    
    def _get_semantic_similarity(self, query1, query2):
        """ë‘ ì§ˆë¬¸ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°"""
        try:
            # ì„ë² ë”© ë²¡í„° ìƒì„±
            emb1 = self.embeddings.embed_query(query1)
            emb2 = self.embeddings.embed_query(query2)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            return similarity
        except:
            return 0.0
    
    def _find_similar_cache(self, query, threshold=0.85):
        """ìœ ì‚¬í•œ ì§ˆë¬¸ì˜ ìºì‹œë¥¼ ì°¾ìŒ"""
        normalized_query = self._normalize_query(query)
        
        # 1. ì •ê·œí™”ëœ ì§ˆë¬¸ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ìºì‹œ ì°¾ê¸°
        for cache_key, cached_data in self.memory_cache.items():
            if 'original_query' in cached_data:
                cached_normalized = self._normalize_query(cached_data['original_query'])
                if cached_normalized == normalized_query:
                    return cached_data
        
        # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ ì°¾ê¸°
        best_match = None
        best_similarity = 0
        
        for cache_key, cached_data in self.memory_cache.items():
            if 'original_query' in cached_data:
                similarity = self._get_semantic_similarity(query, cached_data['original_query'])
                if similarity > threshold and similarity > best_similarity:
                    best_similarity = similarity
                    best_match = cached_data
        
        if best_match:
            print(f"ğŸ¯ ì˜ë¯¸ì  ìœ ì‚¬ ìºì‹œ íˆíŠ¸! ìœ ì‚¬ë„: {best_similarity:.2f}")
            return best_match
        
        return None
    
    def _get_cache_key(self, query, context_type="rag"):
        """ìºì‹œ ì¡°íšŒ"""
        cache_key = f"{query}_{context_type}"
        return cache_key
    
    def _is_expired(self, timestamp):
        now = datetime.now()
        hours = now - timestamp
        if hours > timedelta(hours=12):
            print("ìºì‹œ 12ì‹œê°„ ì´ìƒ ì§€ë‚¨")
            return True
        
        return False
    
    def get(self, query, context_type="rag"):
        """ê°œì„ ëœ ìºì‹œ ì¡°íšŒ - ìœ ì‚¬í•œ ì§ˆë¬¸ë„ ì°¾ìŒ"""
        # 1. ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì •í™•í•œ ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(query, context_type)
        if cache_key in self.memory_cache:
            cached_data = self.memory_cache[cache_key]
            if not self._is_expired(cached_data['timestamp']):
                print(f"ğŸ’¾ ì •í™•í•œ ìºì‹œ íˆíŠ¸: {query[:30]}...")
                return cached_data
            else:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                del self.memory_cache[cache_key]
                self._delete_file(cache_key)

        # 2. íŒŒì¼ ìºì‹œ í™•ì¸
        cached_data = self._load_from_file(cache_key)
        if cached_data and not self._is_expired(cached_data['timestamp']):
            # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ë¡œë“œ
            self.memory_cache[cache_key] = cached_data
            print(f"íŒŒì¼ ìºì‹œ íˆíŠ¸: {query[:30]}...")
            return cached_data
        elif cached_data:  # ë§Œë£Œëœ ê²½ìš°
            self._delete_file(cache_key)
        
        # 3. ìœ ì‚¬í•œ ì§ˆë¬¸ì˜ ìºì‹œ í™•ì¸
        similar_cache = self._find_similar_cache(query)
        if similar_cache and not self._is_expired(similar_cache['timestamp']):
            if similar_cache['context_type'] != context_type:
                print("context ë‹¤ë¦„!!!")
            else:
                return similar_cache
        
        # 4. ë””ìŠ¤í¬ ìºì‹œ í™•ì¸ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)
                
                if not self._is_expired(cached_data['timestamp']):
                    self.memory_cache[cache_key] = cached_data
                    print(f"ğŸ’½ ë””ìŠ¤í¬ ìºì‹œ íˆíŠ¸: {query[:30]}...")
                    return cached_data
                else:
                    os.remove(cache_file)
            except:
                pass
        
        return None
    
    def set(self, query, result, context_type="rag"):
        """ìºì‹œ ì €ì¥ ì‹œ ì›ë³¸ ì§ˆë¬¸ë„ í•¨ê»˜ ì €ì¥"""
        cache_key = self._get_cache_key(query, context_type)
        
        cached_data = {
            'result': result,
            'timestamp': datetime.now(),
            'original_query': query,  # ğŸ”¥ ì›ë³¸ ì§ˆë¬¸ ì €ì¥
            'context_type': context_type
        }
        
        self.memory_cache[cache_key] = cached_data

        # íŒŒì¼ì— ì €ì¥
        self._save_to_file(cache_key, cached_data)

        print(f"ğŸ’¾ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥: {query[:30]}...")
    

# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
cache = SmartCache(cache_dir="lang_cache", expire_hours=12)

# --- 8. LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ ---
def classify_query_node(state: AgentState):
    """ì§ˆë¬¸ ë¶„ë¥˜ ë…¸ë“œ - Agentê°€ í•„ìš”í•œì§€ íŒë‹¨"""
    query = state["query"]
    
    # ê³„ì‚°/ë¶„ì„ì´ í•„ìš”í•œ í‚¤ì›Œë“œë“¤
    analysis_keywords = ['ìµœëŒ€', 'ìµœì†Œ', 'í‰ê· ', 'í•©ê³„', 'ë¶„ì„', 'í†µê³„', 'ê³„ì‚°', 'ë¹„êµ', 'ì´í•©', 'ê°€ì¥']
    # search_keywords = ['ì°¾ì•„', 'ê²€ìƒ‰', 'ì–¸ì œ', 'ì–´ë–¤', 'ëª‡', 'ì–¼ë§ˆ']
    search_keywords = []
    
    # í‚¤ì›Œë“œì— ë”°ë¼ Agent í•„ìš” ì—¬ë¶€ ê²°ì •
    needs_agent = any(keyword in query for keyword in analysis_keywords + search_keywords)
    
    state["needs_agent"] = needs_agent
    print(f"ğŸ” ì§ˆë¬¸ ë¶„ë¥˜: {'Agent í•„ìš”' if needs_agent else 'RAGë§Œ í•„ìš”'}")
    
    return state

from zoneinfo import ZoneInfo
def get_time(state: AgentState):
    """í˜„ì¬ ë…„ë„ì™€ ì›” ê°€ì ¸ì˜¤ê¸°"""
    KST = ZoneInfo('Asia/Seoul')
    y = datetime.now(KST).year
    m = datetime.now(KST).month

    state["current_time"] = f"{y}ë…„ {m}ì›”"
    # print(f'{state["current_time"]}')

    return state

def rag_node(state: AgentState):
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    print("ğŸ“š RAG ê²€ìƒ‰ ì‹œì‘...")

    # ğŸ”¥ ìºì‹œ í™•ì¸ (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
    if state["needs_agent"]:
        cached_result = cache.get(query, "agent")
    else:
        cached_result = cache.get(query, "rag")
    if cached_result:
        print("ğŸ’¾ RAG ìºì‹œ íˆíŠ¸!")
        state["rag_result"] = cached_result["result"]
        state["source_documents"] = cached_result.get("source_docs", [])
        return state  # ğŸš€ LLM í˜¸ì¶œ ì—†ì´ ë°”ë¡œ ë¦¬í„´!
    
    try:
        # ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        history_str = "\n".join([f"Human: {q}\nAI: {a}" for q, a in state["chat_history"]])
        
        # RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        rag_prompt = f"""
            ë‹¹ì‹ ì€ í’ˆëª©ë‹¹(ê°ˆì¹˜, ì˜¤ì§•ì–´, ê³ ë“±ì–´) ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ë§Œì•½ í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´ ì €í¬ëŠ” í’ˆëª©ë‹¹ ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ë§Œ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì´ë¼ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì¤˜.
            í•„ìš”í•˜ë‹¤ë©´ ë‹¤ìŒì˜ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
            ë§Œì•½ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.
            {state["needs_agent"]}ì—ì„œ Trueë¼ë©´ ê³„ì‚°ê³¼ ë¶„ì„ì— í•„ìš”í•œ ì •ë³´ë§Œ ë³´ì—¬ì£¼ì„¸ìš”.

            í˜„ì¬ ì‹œê°: {state["current_time"]}

            ëŒ€í™” ê¸°ë¡:
            {history_str}

            ì§ˆë¬¸: {state["query"]}
        """

        rag_prompt = f"""
            You are a MySQL's SQL generator. 
            Given a request, output ONLY a valid SQL query. 
            Do not include explanations, comments, or any other text.
            limit 1ì„ ì¨ì•¼í•  ê²½ìš° ëŒ€ì‹  limit 3ë¥¼ ì”ë‹ˆë‹¤.
            ì¿¼ë¦¬ë¬¸ì´ ë‘ ê°œ ì´ìƒì¼ ë•Œ í•˜ë‚˜ì˜ ì¿¼ë¦¬ë¬¸ìœ¼ë¡œ í•©ì³ì„œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
            í•˜ë‹¨ì€ ì˜ˆì‹œì…ë‹ˆë‹¤. ê°™ì€ tableê³¼ joinì„ ì‚¬ìš©í•˜ê³  ì¶œë ¥ì˜ í•­ëª©ì€ ë™ì¼í•©ë‹ˆë‹¤.
            SELECT 
                CASE 
                    WHEN i.item_name = 'Calamari' THEN 'ì˜¤ì§•ì–´'
                    WHEN i.item_name = 'CutlassFish' THEN 'ê°ˆì¹˜'
                    WHEN i.item_name = 'Mackerel' THEN 'ê³ ë“±ì–´'
                    ELSE i.item_name
                END AS item_name,
            ir.month_date, ir.production, ir.inbound, ir.sales
            FROM item_retail ir
            LEFT JOIN item i ON ir.item_pk = i.item_pk
            
            í˜„ì¬ ì‹œê°: {state["current_time"]}

            ëŒ€í™” ê¸°ë¡:
            {history_str}

            ì§ˆë¬¸: {state["query"]}
        """
        
        response = qa_chain.invoke({"query": rag_prompt})
        rag_sql = response['result']
        print(rag_sql)
        cursor.execute(rag_sql)
        rows = cursor.fetchall()

        texts = [f"""í’ˆëª©: {row[0]}, ë‚ ì§œ: {row[1]}, ìƒì‚°ëŸ‰: {row[2]}, ìˆ˜ì…ëŸ‰: {row[3]}, íŒë§¤ëŸ‰: {row[4]}
                """
                for row in rows]
        state["rag_result"] = texts

        # response = qa_chain.invoke({"query": rag_prompt})
        # state["rag_result"] = response["result"]
        state["source_documents"] = response.get('source_documents', [])
        
        print(f"âœ… RAG ê²€ìƒ‰ ì™„ë£Œ: {len(state['source_documents'])}ê°œ ë¬¸ì„œ ì°¸ì¡°")

        cache.set(query=query, result=response['result'], context_type="rag")
        
    except Exception as e:
        print(f"âŒ RAG ì˜¤ë¥˜: {e}")
        if "Error code: 429" in str(e) and model.openai_api_key != openai_api_key2:
            model.openai_api_key = openai_api_key2
            llm_math.llm.openai_api_key  = openai_api_key2
            rag_node(state)
        state["rag_result"] = f"RAG ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        state["source_documents"] = []
    
    return state

def agent_node(state: AgentState):
    """Agent ë¶„ì„ ë…¸ë“œ"""
    print("ğŸ¤– Agent ë¶„ì„ ì‹œì‘...")

    # ğŸ”¥ ë³µí•© ìºì‹œ í‚¤ ìƒì„± (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
    cached_result = cache.get(query, "agent")

    if cached_result:
        print("ğŸ’¾ Agent ìºì‹œ íˆíŠ¸!")
        state["agent_result"] = cached_result["result"]
        return state  # ğŸš€ Agent ì‹¤í–‰ ì—†ì´ ë°”ë¡œ ë¦¬í„´!
    
    try:
        # RAG ê²°ê³¼ë¥¼ í¬í•¨í•œ ì…ë ¥ êµ¬ì„±
        # RAG ê²€ìƒ‰ ê²°ê³¼: {state["rag_result"]}
        # RAG ê²€ìƒ‰ ê²°ê³¼: {texts}

        # ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        history_str = "\n".join([f"Human: {q}\nAI: {a}" for q, a in state["chat_history"]])

        # ë§Œì•½ ì¶”ê°€ì ì¸ ë°ì´í„° ë¶„ì„ì´ í•„ìš”í•˜ë‹¤ë©´ analyze_data ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê³ ,
        agent_input = f"""
        ì•„ë˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•„ìš”í•œ ë¶„ì„ì´ë‚˜ ê³„ì‚°ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
        ì—¬ëŸ¬ í’ˆëª©ì´ ìˆì§€ ì•Šê±°ë‚˜ ë°ì´í„°ê°€ ë§ì§€ ì•Šë”ë¼ë„ ê²€ìƒ‰ ê²°ê³¼ëŠ” í•œë²ˆ ì •ì œí•´ì„œ ì£¼ëŠ” ë°ì´í„°ì…ë‹ˆë‹¤. ë¶€ì¡±í•˜ì§€ ì•Šìœ¼ë‹ˆ ê·¸ê±¸ë¡œ ëŒ€ë‹µí•´.
        ìˆ˜í•™ ê³„ì‚°ì´ í•„ìš”í•˜ë‹¤ë©´ calculator ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
        í˜„ì¬ ì‹œê°: {state["current_time"]}

        ê²€ìƒ‰ ê²°ê³¼: {state["rag_result"]}
        
        ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸: {query}

        ëŒ€í™” ê¸°ë¡:
        {history_str}
        """
        
        response = agent_executor.invoke({"input": agent_input})
        state["agent_result"] = response['output']
        
        print("âœ… Agent ë¶„ì„ ì™„ë£Œ")

        cache.set(query=query, result=response['output'], context_type="agent")
        
    except Exception as e:
        print(f"âŒ Agent ì˜¤ë¥˜: {e}")
        if "Error code: 429" in str(e) and model.openai_api_key != openai_api_key2:
            model.openai_api_key = openai_api_key2
            llm_math.llm.openai_api_key  = openai_api_key2
            rag_node(state)
        state["agent_result"] = f"Agent ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
    
    return state

def combine_results_node(state: AgentState):
    """ê²°ê³¼ í†µí•© ë…¸ë“œ"""
    print("ğŸ”— ê²°ê³¼ í†µí•© ì¤‘...")
    
    if state["needs_agent"] and state["agent_result"] and "ì˜¤ë¥˜" not in state["agent_result"]:
        # Agent ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        state["final_answer"] = f"""{state["agent_result"]}

RAG:
{state["rag_result"]}
"""
    else:
        # RAGë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        state["final_answer"] = state["rag_result"]

    # # ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    # history_str = "\n".join([f"Human: {q}\nAI: {a}" for q, a in state["chat_history"]])

    # prompt = f'''
    #     í•„ìš”í•˜ë‹¤ë©´ ë‹¤ìŒì˜ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    #     ê²€ìƒ‰ ê²°ê³¼: {state["final_answer"]}
        
    #     ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸: {query}

    #     ëŒ€í™” ê¸°ë¡:
    #     {history_str}
    # '''

    # sys_prompt = f'''
    #     ë‹¹ì‹ ì€ í’ˆëª©ë‹¹(ê°ˆì¹˜, ì˜¤ì§•ì–´, ê³ ë“±ì–´) ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    #     ë§Œì•½ í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´ ì €í¬ëŠ” í’ˆëª©ë‹¹ ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ë§Œ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì´ë¼ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì¤˜.
    #     í•„ìš”í•˜ë‹¤ë©´ ë‹¤ìŒì˜ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    #     ë§Œì•½ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.
    # '''

    # response = model.invoke([{'role':'user', 'content':prompt},
    #                         {'role':'system', 'content':sys_prompt}])

    # state["final_answer"] = response.content
    
    print("âœ… ê²°ê³¼ í†µí•© ì™„ë£Œ")
    return state

# --- 9. ë¼ìš°íŒ… í•¨ìˆ˜ ---
def decide_next_step(state: AgentState):
    """ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°íŒ… í•¨ìˆ˜"""
    if state["needs_agent"]:
        return "agent"
    else:
        return "rag"

# --- 10. LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ---
workflow = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("classify", classify_query_node)
workflow.add_node("time", get_time)
workflow.add_node("rag", rag_node)
workflow.add_node("agent", agent_node)
workflow.add_node("combine", combine_results_node)

# ì—£ì§€ ì¶”ê°€
def workflow_add_edge_1():
    workflow.add_edge(START, "classify")
    workflow.add_edge("classify", "time")
    workflow.add_edge("time", "rag")
    workflow.add_conditional_edges(
        "rag",
        decide_next_step,
        {
            "agent": "agent",
            "rag": "combine"
        }
    )
    workflow.add_edge("agent", "combine")
    workflow.add_edge("combine", END)

def workflow_add_edge_2():
    workflow.add_edge(START, "classify")
    workflow.add_edge("classify", "time")
    workflow.add_conditional_edges(
        "time",
        decide_next_step,
        {
            "agent": "agent",
            "rag": "rag"
        }
    )
    workflow.add_edge("agent", "combine")
    workflow.add_edge("rag", "combine")
    workflow.add_edge("combine", END)

def workflow_add_edge_3():
    workflow.add_edge(START, "classify")
    workflow.add_edge("classify", "time")
    workflow.add_edge("time", "rag")
    workflow.add_edge("rag", "agent")
    workflow.add_edge("agent", "combine")
    workflow.add_edge("combine", END)

workflow_add_edge_1()

# ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼
app = workflow.compile()
def lang_graph_png():
    from IPython.display import Image, display

    try:
        #ìš°ë¦¬ê°€ ë§Œë“  appì˜ graphë¥¼ ì´ë¯¸ì§€ë¡œ ê·¸ë ¤ì„œ display()ë¡œ ê°ì‹¸ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
        with open("LangGraph.png", "wb") as f:
            f.write(app.get_graph().draw_mermaid_png())
        # display(Image("LangGraph.png"))

    except Exception:
        print("ë­ê·¸ë˜í”„ png ì‹¤íŒ¨")

# lang_graph_png()

print("ğŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ì¤€ë¹„ ì™„ë£Œ!")

# --- 11. ë©”ì¸ ëŒ€í™” ë£¨í”„ ---
chat_history = []

while True:
    query = input("\nì§ˆë¬¸í•´ì£¼ì„¸ìš”. >> ")
    if query.lower() == "exit":
        break
    
    print("\n" + "="*60)
    print(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {query}")
    print("="*60)
    
    # LangGraph ì‹¤í–‰
    try:
        result = app.invoke({
            "query": query,
            "chat_history": chat_history,
            "current_time": "",
            "rag_result": "",
            "agent_result": "",
            "final_answer": "",
            "needs_agent": False,
            "source_documents": []
        })
        
        # ê²°ê³¼ ì¶œë ¥
        final_answer = result['final_answer']
        source_docs = result['source_documents']
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ (ìµœëŒ€ 3ê°œ ìœ ì§€)
        chat_history.append((query, final_answer))
        if len(chat_history) > 3:
            chat_history.pop(0)
        
        # ê²°ê³¼ í‘œì‹œ
        print("\n" + "-"*50)
        if source_docs:
            print(f"ğŸ“Š ì°¸ì¡°ëœ ë¬¸ì„œ ìˆ˜: {len(source_docs)}ê°œ")
        print(f"ğŸ’¬ ë‹µë³€:\n{final_answer}")
        print("="*50)

        # if cache.get(query, "rag"):
        #     print(f"ğŸ’¾ ìºì‹œ í™œìš©ìœ¼ë¡œ í† í° ì ˆì•½!")  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
        # else:
        #     print(f"ìºì‹œ ì¶”ê°€!!!")
        
    except Exception as e:
        print(f"âŒ ì „ì²´ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        print("="*50)

# ê°ˆì¹˜ì˜ í‰ê·  ìƒì‚°ëŸ‰
# ì´ 9693
# í‰ê·  1,384.714285

# ì˜¤ì§•ì–´ì˜ í‰ê·  ìƒì‚°ëŸ‰
# ì´ 6435
# í‰ê·  919.285714

# ê³ ë“±ì–´ì˜ í‰ê·  ìƒì‚°ëŸ‰
# ì´ 60,483
# í‰ê·  8,640.4285