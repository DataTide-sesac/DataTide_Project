import mysql.connector
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# ì±„íŒ…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì–‘ì‹ì„ ë§Œë“¦
from langchain.prompts import ChatPromptTemplate
from langchain.agents import AgentExecutor, create_tool_calling_agent
import langchain

# ì‹œê°„ ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime
import os
import json, requests
from tempfile import tempdir
from dotenv import load_dotenv

from langgraph.graph import StateGraph, START, END  # ìƒíƒœê·¸ë˜í”„ì™€ ENDë…¸ë“œ ì„¸íŒ…
from typing_extensions import TypedDict             # ìƒíƒœê·¸ë˜í”„ ì»¤ìŠ¤í„°ë§ˆì´ì§•
from langchain.agents import Tool
from langchain.chains import LLMMathChain

# tool ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ì¼ë°˜ í•¨ìˆ˜ë¥¼ 'íˆ´'ë¡œ ì¸ì§€ì‹œí‚¤ê¸° ìœ„í•¨
from langchain.tools import tool
#ë§Œë“  íˆ´ì„ ì‚¬ìš©í•˜ì!
from langchain.chat_models import init_chat_model


# --- í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° ---
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "../..", ".env"))
openai_api_key = os.getenv("OPENAI_API_KEY")

mysql_user = os.getenv("MYSQL_USER")
mysql_password = os.getenv("MYSQL_PASSWORD")
mysql_database = os.getenv("MYSQL_DATABASE")

# --- 1. MySQL ì—°ê²° ---
conn = mysql.connector.connect(
    host="localhost",
    user=mysql_user,
    password=mysql_password,
    database=mysql_database
)

# ë””ë¹„ì—ì„œ ì½ê¸°
cursor = conn.cursor()
texts = []
def read_sql_1():
    sql = """
        SELECT 
            CASE 
                WHEN i.item_name = 'Calamari' THEN 'ì˜¤ì§•ì–´'
                WHEN i.item_name = 'CutlassFish' THEN 'ê°ˆì¹˜'
                WHEN i.item_name = 'Mackerel' THEN 'ê³ ë“±ì–´'
                ELSE i.item_name
            END AS item_name,
        ir.month_date, ir.production, ir.inbound, ir.sales, 
        gw.temperature as gw_temp, gw.rain as gw_rain,
        l.local_name,
        sw.temperature as sw_temp, sw.wind, sw.salinity, sw.wave_height, sw.wave_period, sw.wave_speed, sw.rain as sw_rain, sw.snow as sw_snow
        FROM item_retail ir
        LEFT JOIN sea_weather sw ON ir.month_date = sw.month_date
        LEFT JOIN ground_weather gw ON ir.month_date = gw.month_date
        LEFT JOIN location l ON sw.local_pk = l.local_pk
        LEFT JOIN item i ON ir.item_pk = i.item_pk
    """
    cursor.execute(sql)
    rows = cursor.fetchall()

    texts = [f"""í’ˆëª©: {row[0]}, ë‚ ì§œ: {row[1]}, ìƒì‚°ëŸ‰: {row[2]}, ìˆ˜ì…ëŸ‰: {row[3]}, íŒë§¤ëŸ‰: {row[4]},
            ì „êµ­ ì§€ìƒ ë‚ ì”¨: {row[5]}, ì „êµ­ ì§€ìƒ ê°•ìˆ˜ëŸ‰: {row[6]}, ì§€ì—­: {row[7]},
            í•´ì–‘ ë‚ ì”¨: {row[8]}, ì—¼ë¶„: {row[9]}, ìœ ì˜íŒŒê³ : {row[10]}, ìœ ì˜íŒŒì£¼ê¸°: {row[11]}, ìœ ì†: {row[12]}, í•´ì–‘ ê°•ìˆ˜ëŸ‰: {row[13]}, í•´ì–‘ ì ì„¤ëŸ‰: {row[14]}
            """
            for row in rows]
    
def read_sql_2():
    sql = """
        SELECT 
            CASE 
                WHEN i.item_name = 'Calamari' THEN 'ì˜¤ì§•ì–´'
                WHEN i.item_name = 'CutlassFish' THEN 'ê°ˆì¹˜'
                WHEN i.item_name = 'Mackerel' THEN 'ê³ ë“±ì–´'
                ELSE i.item_name
            END AS item_name,
        ir.month_date, ir.production, ir.inbound, ir.sales
        FROM item_retail ir
        LEFT JOIN item i ON ir.item_pk = i.item_pk
    """
    cursor.execute(sql)
    rows = cursor.fetchall()

    texts = [f"""í’ˆëª©: {row[0]}, ë‚ ì§œ: {row[1]}, ìƒì‚°ëŸ‰: {row[2]}, ìˆ˜ì…ëŸ‰: {row[3]}, íŒë§¤ëŸ‰: {row[4]}
            """
            for row in rows]
    
    return texts
    
texts = read_sql_2()

# --- 2. ì²­í¬í™” ---
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)
chunks = []
for text in texts:
    chunks.extend(splitter.split_text(text))

# --- 3. ì„ë² ë”© & ë²¡í„° DB ---
embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-m3')

if os.path.exists("faiss_index"):
    vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
else:
    vectorstore = FAISS.from_texts(chunks, embeddings)
    vectorstore.save_local("faiss_index")

print("ì„ë² ë”© & FAISS DB ì¤€ë¹„ ì™„ë£Œ!")

# --- 4. LLM ì¤€ë¹„ ---
model = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0,
    openai_api_key=openai_api_key,
    max_tokens=256  # ë‹µë³€ ê¸¸ì´ë¥¼ ëŠ˜ë ¤ì¤ë‹ˆë‹¤.
)

# ë™ì¼í•œ ì„¸íŒ…ì˜ llmì„ LLMMathChainì—ê²Œ ì „ë‹¬
llm_math = LLMMathChain(llm=ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
        openai_api_key=openai_api_key,
        max_tokens=256  # ë‹µë³€ ê¸¸ì´ë¥¼ ëŠ˜ë ¤ì¤ë‹ˆë‹¤.
    ))

# ìˆ˜í•™ì  íˆ´
math_tool = Tool(
    name = 'calculator',
    func = llm_math.run,
    description = 'ìˆ˜í•™ ê³„ì‚°ê³¼ ê´€ë ¨ëœ ë¬¸ì œì— ëŒ€í•´ í•„ìš”í•˜ë©´ ì´ ë„êµ¬ë¥¼ ì“°ì‹œì˜¤.'
)

# --- 5. ì¶”ê°€ ë°ì´í„° ë¶„ì„ ë„êµ¬ë“¤ ---
@tool
def analyze_data(query: str) -> str:
    """RAG ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ìµœëŒ€ê°’, ìµœì†Œê°’, í‰ê· , í•©ê³„ ë“±ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    try:
        import re
        import statistics
        from collections import defaultdict
        
        # ë²¡í„° ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        docs = vectorstore.similarity_search(query, k=100)
        
        # ìˆ«ì ë°ì´í„° ì¶”ì¶œ
        data_dict = defaultdict(list)
        
        for doc in docs:
            content = doc.page_content
            
            # ìƒì‚°ëŸ‰, ìˆ˜ì…ëŸ‰, íŒë§¤ëŸ‰ ë“±ì˜ ìˆ«ì ë°ì´í„° ì¶”ì¶œ
            patterns = {
                'ìƒì‚°ëŸ‰': r'ìƒì‚°ëŸ‰:\s*([0-9.]+)',
                'ìˆ˜ì…ëŸ‰': r'ìˆ˜ì…ëŸ‰:\s*([0-9.]+)',
                'íŒë§¤ëŸ‰': r'íŒë§¤ëŸ‰:\s*([0-9.]+)'
            }
            
            for key, pattern in patterns.items():
                matches = re.findall(pattern, content)
                for match in matches:
                    try:
                        value = float(match)
                        if value > 0:  # 0ë³´ë‹¤ í° ê°’ë§Œ í¬í•¨
                            data_dict[key].append(value)
                    except ValueError:
                        continue
        
        # ë¶„ì„ ê²°ê³¼ ìƒì„±
        result = "=== ë°ì´í„° ë¶„ì„ ê²°ê³¼ ===\n"
        
        for key, values in data_dict.items():
            if values:
                result += f"\nã€{key}ã€‘\n"
                result += f"  ë°ì´í„° ê°œìˆ˜: {len(values)}ê°œ\n"
                result += f"  ìµœëŒ€ê°’: {max(values):.2f}\n"
                result += f"  ìµœì†Œê°’: {min(values):.2f}\n"
                result += f"  í‰ê· ê°’: {statistics.mean(values):.2f}\n"
                result += f"  í•©ê³„: {sum(values):.2f}\n"
                
                if len(values) > 1:
                    result += f"  í‘œì¤€í¸ì°¨: {statistics.stdev(values):.2f}\n"
        
        return result if result != "=== ë°ì´í„° ë¶„ì„ ê²°ê³¼ ===\n" else "ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìˆ«ì ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    except Exception as e:
        return f"ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

# tools = [math_tool]
tools = [math_tool, analyze_data]
print(tools[0].name, tools[0].description)

# model(ìƒê°í•  llm, agentì˜ ë‘ë‡Œ),
# tools(agentê°€ ì‚¬ìš©ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡),
# prompt(agentê°€ íŒë‹¨í•  ë•Œ ì–´ë–¤ ê¸°ì¤€ì´ ìˆë‹¤ë©´? ì´ê±¸ í”„ë¡¬í”„íŠ¸ê°€ ë‹´ê³  ìˆë‹¤.)
prompt = ChatPromptTemplate.from_messages([
    ('system', '''ë‹¹ì‹ ì€ í’ˆëª©ë‹¹ ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ë§Œì•½ í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´ ì €í¬ëŠ” í’ˆëª©ë‹¹ ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ë§Œ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì´ë¼ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì¤˜.
    ëŒ€ë‹µì€ ì¹œì ˆí•˜ê²Œ í•´ì£¼ì„¸ìš”.
    ë‹¤ìŒ ë„êµ¬ë“¤ì„ í™œìš©í•  ìˆ˜ ìˆì–´:
    1. calculator: ìˆ˜í•™ ê³„ì‚°
    2. analyze_data: ë°ì´í„° ë¶„ì„ ë° í†µê³„ ê³„ì‚°
    
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•´ì„œ ì‚¬ìš©í•´ì¤˜.'''),
    ('human', '{input}'),
    ('placeholder', '{agent_scratchpad}')   # toolsì„ ì‚¬ìš©í•˜ë©´ì„œ ë‚¨ê¸°ëŠ” ì¤‘ê°„ ì‘ì—… ë‚´ì—­
])

# agentë¥¼ ë§Œë“¦ => ì´ agentëŠ” toolì„ calling =>
# model(ë‘ë‡Œ)ë¡œ tools(ë„êµ¬)ë¥¼ prompt(ì§€ì‹œì‚¬í•­)ì— ë§ê²Œ íŒë‹¨í•´ì„œ ë¶€ë¦„
agent = create_tool_calling_agent(model, tools, prompt)

# ì—ì´ì „íŠ¸ì˜ ì‹¤í–‰
agent_executor = AgentExecutor(agent=agent, tools=tools)
# result = agent_executor.invoke({'input':'what is 2.1^2.1'})

# --- 6. RetrievalQA ì²´ì¸ êµ¬ì„± ---
qa_chain = RetrievalQA.from_chain_type(
    llm=model,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 10}),
    return_source_documents=True,
)

# --- 7. LangGraph State ì •ì˜ ---
class AgentState(TypedDict):
    query: str
    chat_history: list
    current_time: str
    rag_result: str
    agent_result: str
    final_answer: str
    needs_agent: bool
    source_documents: list

# --- 8. LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ ---
def classify_query_node(state: AgentState):
    """ì§ˆë¬¸ ë¶„ë¥˜ ë…¸ë“œ - Agentê°€ í•„ìš”í•œì§€ íŒë‹¨"""
    query = state["query"]
    
    # ê³„ì‚°/ë¶„ì„ì´ í•„ìš”í•œ í‚¤ì›Œë“œë“¤
    analysis_keywords = ['ìµœëŒ€', 'ìµœì†Œ', 'í‰ê· ', 'í•©ê³„', 'ë¶„ì„', 'í†µê³„', 'ê³„ì‚°', 'ë¹„êµ', 'ì´í•©']
    search_keywords = ['ì°¾ì•„', 'ê²€ìƒ‰', 'ì–¸ì œ', 'ì–´ë–¤', 'ëª‡', 'ì–¼ë§ˆ', 'ê°€ì¥']
    
    # í‚¤ì›Œë“œì— ë”°ë¼ Agent í•„ìš” ì—¬ë¶€ ê²°ì •
    needs_agent = any(keyword in query for keyword in analysis_keywords + search_keywords)
    
    state["needs_agent"] = needs_agent
    print(f"ğŸ” ì§ˆë¬¸ ë¶„ë¥˜: {'Agent í•„ìš”' if needs_agent else 'RAGë§Œ í•„ìš”'}")
    
    return state

from zoneinfo import ZoneInfo
def get_time(state: AgentState):
    """í˜„ì¬ ë…„ë„ì™€ ì›” ê°€ì ¸ì˜¤ê¸°"""
    KST = ZoneInfo('Asia/Seoul')
    y = datetime.now(KST).year
    m = datetime.now(KST).month

    state["current_time"] = f"{y}ë…„ {m}ì›”"
    # print(f'{state["current_time"]}')

    return state


def rag_node(state: AgentState):
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    print("ğŸ“š RAG ê²€ìƒ‰ ì‹œì‘...")
    
    try:
        # ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        history_str = "\n".join([f"Human: {q}\nAI: {a}" for q, a in state["chat_history"]])
        
        # RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        rag_prompt = f"""
            ë‹¹ì‹ ì€ í’ˆëª©ë‹¹ ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ë§Œì•½ í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´ ì €í¬ëŠ” í’ˆëª©ë‹¹ ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ë§Œ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì´ë¼ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì¤˜.
            ëŒ€ë‹µì€ ì¹œì ˆí•˜ê²Œ í•´ì£¼ì„¸ìš”.
            í•„ìš”í•˜ë‹¤ë©´ ë‹¤ìŒì˜ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
            ë§Œì•½ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.

            í˜„ì¬ ì‹œê°: {state["current_time"]}

            ëŒ€í™” ê¸°ë¡:
            {history_str}

            ì§ˆë¬¸: {state["query"]}
        """
        
        response = qa_chain.invoke({"query": rag_prompt})
        state["rag_result"] = response['result']
        state["source_documents"] = response.get('source_documents', [])
        
        print(f"âœ… RAG ê²€ìƒ‰ ì™„ë£Œ: {len(state['source_documents'])}ê°œ ë¬¸ì„œ ì°¸ì¡°")
        
    except Exception as e:
        print(f"âŒ RAG ì˜¤ë¥˜: {e}")
        state["rag_result"] = f"RAG ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        state["source_documents"] = []
    
    return state

def agent_node(state: AgentState):
    """Agent ë¶„ì„ ë…¸ë“œ"""
    print("ğŸ¤– Agent ë¶„ì„ ì‹œì‘...")
    
    try:
        # RAG ê²°ê³¼ë¥¼ í¬í•¨í•œ ì…ë ¥ êµ¬ì„±
        # RAG ê²€ìƒ‰ ê²°ê³¼: {state["rag_result"]}
        # RAG ê²€ìƒ‰ ê²°ê³¼: {texts}
        agent_input = f"""
        í˜„ì¬ ì‹œê°: {state["current_time"]}

        ê²€ìƒ‰ ê²°ê³¼: {texts}
        
        ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸: {state["query"]}
        
        ìœ„ RAG ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•„ìš”í•œ ë¶„ì„ì´ë‚˜ ê³„ì‚°ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ì¶”ê°€ì ì¸ ë°ì´í„° ë¶„ì„ì´ í•„ìš”í•˜ë‹¤ë©´ analyze_data ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê³ ,
        ìˆ˜í•™ ê³„ì‚°ì´ í•„ìš”í•˜ë‹¤ë©´ calculator ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
        """
        
        response = agent_executor.invoke({"input": agent_input})
        state["agent_result"] = response['output']
        
        print("âœ… Agent ë¶„ì„ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ Agent ì˜¤ë¥˜: {e}")
        state["agent_result"] = f"Agent ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
    
    return state

def combine_results_node(state: AgentState):
    """ê²°ê³¼ í†µí•© ë…¸ë“œ"""
    print("ğŸ”— ê²°ê³¼ í†µí•© ì¤‘...")
    
    if state["needs_agent"] and state["agent_result"] and "ì˜¤ë¥˜" not in state["agent_result"]:
        # Agent ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        state["final_answer"] = f"""{state["agent_result"]}

{state["rag_result"]}"""
    else:
        # RAGë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        state["final_answer"] = state["rag_result"]
    
    print("âœ… ê²°ê³¼ í†µí•© ì™„ë£Œ")
    return state

# --- 9. ë¼ìš°íŒ… í•¨ìˆ˜ ---
def decide_next_step(state: AgentState):
    """ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°íŒ… í•¨ìˆ˜"""
    if state["needs_agent"]:
        return "agent"
    else:
        return "combine"

# --- 10. LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ---
workflow = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("classify", classify_query_node)
workflow.add_node("time", get_time)
workflow.add_node("rag", rag_node)
workflow.add_node("agent", agent_node)
workflow.add_node("combine", combine_results_node)

# ì—£ì§€ ì¶”ê°€
def workflow_add_edge_2():
    workflow.add_edge(START, "classify")
    workflow.add_edge("classify", "time")
    workflow.add_conditional_edges(
        "time",
        decide_next_step,
        {
            "agent": "agent",
            "combine": "rag"
        }
    )
    workflow.add_edge("agent", "combine")
    workflow.add_edge("rag", "combine")
    workflow.add_edge("combine", END)

workflow_add_edge_2()

# ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼
app = workflow.compile()

print("ğŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ì¤€ë¹„ ì™„ë£Œ!")

# --- 11. ë©”ì¸ ëŒ€í™” ë£¨í”„ ---
chat_history = []

while True:
    query = input("\nì§ˆë¬¸í•´ì£¼ì„¸ìš”. >> ")
    if query.lower() == "exit":
        break
    
    print("\n" + "="*60)
    print(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {query}")
    print("="*60)
    
    # LangGraph ì‹¤í–‰
    try:
        result = app.invoke({
            "query": query,
            "chat_history": chat_history,
            "rag_result": "",
            "agent_result": "",
            "final_answer": "",
            "needs_agent": False,
            "source_documents": []
        })
        
        # ê²°ê³¼ ì¶œë ¥
        final_answer = result['final_answer']
        source_docs = result['source_documents']
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ (ìµœëŒ€ 3ê°œ ìœ ì§€)
        chat_history.append((query, final_answer))
        if len(chat_history) > 3:
            chat_history.pop(0)
        
        # ê²°ê³¼ í‘œì‹œ
        print("\n" + "-"*50)
        if source_docs:
            print(f"ğŸ“Š ì°¸ì¡°ëœ ë¬¸ì„œ ìˆ˜: {len(source_docs)}ê°œ")
        print(f"ğŸ’¬ ë‹µë³€:\n{final_answer}")
        print("="*50)
        
    except Exception as e:
        print(f"âŒ ì „ì²´ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        print("="*50)

# ê°ˆì¹˜ì˜ í‰ê·  ìƒì‚°ëŸ‰
# ì´ 9693
# í‰ê·  1,384.714285

# ì˜¤ì§•ì–´ì˜ í‰ê·  ìƒì‚°ëŸ‰
# ì´ 6435
# í‰ê·  919.285714

# ê³ ë“±ì–´ì˜ í‰ê·  ìƒì‚°ëŸ‰
# ì´ 60,483
# í‰ê·  8,640.4285