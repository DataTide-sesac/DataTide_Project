import mysql.connector
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# ì±„íŒ…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì–‘ì‹ì„ ë§Œë“¦
from langchain.prompts import ChatPromptTemplate
from langchain.agents import AgentExecutor, create_tool_calling_agent
import langchain

# ì‹œê°„ ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime, timedelta
import os, pickle, time
import numpy as np
import json, requests
from tempfile import tempdir
from dotenv import load_dotenv

from langgraph.graph import StateGraph, START, END  # ìƒíƒœê·¸ë˜í”„ì™€ ENDë…¸ë“œ ì„¸íŒ…
from typing_extensions import TypedDict             # ìƒíƒœê·¸ë˜í”„ ì»¤ìŠ¤í„°ë§ˆì´ì§•
from langchain.agents import Tool
from langchain.chains import LLMMathChain

# tool ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ì¼ë°˜ í•¨ìˆ˜ë¥¼ 'íˆ´'ë¡œ ì¸ì§€ì‹œí‚¤ê¸° ìœ„í•¨
from langchain.tools import tool
#ë§Œë“  íˆ´ì„ ì‚¬ìš©í•˜ì!
from langchain.chat_models import init_chat_model


# --- í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° ---
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "../..", ".env"))
openai_api_key = os.getenv("OPENAI_API_KEY")

mysql_user = os.getenv("MYSQL_USER")
mysql_password = os.getenv("MYSQL_PASSWORD")
mysql_database = os.getenv("MYSQL_DATABASE")

# --- 1. MySQL ì—°ê²° ---
conn = mysql.connector.connect(
    host="localhost",
    user=mysql_user,
    password=mysql_password,
    database=mysql_database
)

# ë””ë¹„ì—ì„œ ì½ê¸°
cursor = conn.cursor()
texts = []
def read_sql_1():
    sql = """
        SELECT 
            CASE 
                WHEN i.item_name = 'Calamari' THEN 'ì˜¤ì§•ì–´'
                WHEN i.item_name = 'CutlassFish' THEN 'ê°ˆì¹˜'
                WHEN i.item_name = 'Mackerel' THEN 'ê³ ë“±ì–´'
                ELSE i.item_name
            END AS item_name,
        ir.month_date, ir.production, ir.inbound, ir.sales, 
        gw.temperature as gw_temp, gw.rain as gw_rain,
        l.local_name,
        sw.temperature as sw_temp, sw.wind, sw.salinity, sw.wave_height, sw.wave_period, sw.wave_speed, sw.rain as sw_rain, sw.snow as sw_snow
        FROM item_retail ir
        LEFT JOIN sea_weather sw ON ir.month_date = sw.month_date
        LEFT JOIN ground_weather gw ON ir.month_date = gw.month_date
        LEFT JOIN location l ON sw.local_pk = l.local_pk
        LEFT JOIN item i ON ir.item_pk = i.item_pk
    """
    cursor.execute(sql)
    rows = cursor.fetchall()

    texts = [f"""í’ˆëª©: {row[0]}, ë‚ ì§œ: {row[1]}, ìƒì‚°ëŸ‰: {row[2]}, ìˆ˜ì…ëŸ‰: {row[3]}, íŒë§¤ëŸ‰: {row[4]},
            ì „êµ­ ì§€ìƒ ë‚ ì”¨: {row[5]}, ì „êµ­ ì§€ìƒ ê°•ìˆ˜ëŸ‰: {row[6]}, ì§€ì—­: {row[7]},
            í•´ì–‘ ë‚ ì”¨: {row[8]}, ì—¼ë¶„: {row[9]}, ìœ ì˜íŒŒê³ : {row[10]}, ìœ ì˜íŒŒì£¼ê¸°: {row[11]}, ìœ ì†: {row[12]}, í•´ì–‘ ê°•ìˆ˜ëŸ‰: {row[13]}, í•´ì–‘ ì ì„¤ëŸ‰: {row[14]}
            """
            for row in rows]
    
def read_sql_2():
    sql = """
        SELECT 
            CASE 
                WHEN i.item_name = 'Calamari' THEN 'ì˜¤ì§•ì–´'
                WHEN i.item_name = 'CutlassFish' THEN 'ê°ˆì¹˜'
                WHEN i.item_name = 'Mackerel' THEN 'ê³ ë“±ì–´'
                ELSE i.item_name
            END AS item_name,
        ir.month_date, ir.production, ir.inbound, ir.sales
        FROM item_retail ir
        LEFT JOIN item i ON ir.item_pk = i.item_pk
    """
    cursor.execute(sql)
    rows = cursor.fetchall()

    texts = [f"""í’ˆëª©: {row[0]}, ë‚ ì§œ: {row[1]}, ìƒì‚°ëŸ‰: {row[2]}, ìˆ˜ì…ëŸ‰: {row[3]}, íŒë§¤ëŸ‰: {row[4]}
            """
            for row in rows]
    
    return texts
    
texts = read_sql_2()

# --- 2. ì²­í¬í™” ---
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)
chunks = []
for text in texts:
    chunks.extend(splitter.split_text(text))

# --- 3. ì„ë² ë”© & ë²¡í„° DB ---
embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-m3')

if os.path.exists("faiss_index"):
    vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
else:
    vectorstore = FAISS.from_texts(chunks, embeddings)
    vectorstore.save_local("faiss_index")

print("ì„ë² ë”© & FAISS DB ì¤€ë¹„ ì™„ë£Œ!")

# --- 4. LLM ì¤€ë¹„ ---
model = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0,
    openai_api_key=openai_api_key,
    max_tokens=256  # ë‹µë³€ ê¸¸ì´ë¥¼ ëŠ˜ë ¤ì¤ë‹ˆë‹¤.
)

# ë™ì¼í•œ ì„¸íŒ…ì˜ llmì„ LLMMathChainì—ê²Œ ì „ë‹¬
llm_math = LLMMathChain(llm=ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
        openai_api_key=openai_api_key,
        max_tokens=256  # ë‹µë³€ ê¸¸ì´ë¥¼ ëŠ˜ë ¤ì¤ë‹ˆë‹¤.
    ))

# ìˆ˜í•™ì  íˆ´
math_tool = Tool(
    name = 'calculator',
    func = llm_math.run,
    description = 'ìˆ˜í•™ ê³„ì‚°ê³¼ ê´€ë ¨ëœ ë¬¸ì œì— ëŒ€í•´ í•„ìš”í•˜ë©´ ì´ ë„êµ¬ë¥¼ ì“°ì‹œì˜¤.'
)

# --- 5. ì¶”ê°€ ë°ì´í„° ë¶„ì„ ë„êµ¬ë“¤ ---
@tool
def analyze_data(query: str) -> str:
    """RAG ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ìµœëŒ€ê°’, ìµœì†Œê°’, í‰ê· , í•©ê³„ ë“±ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""
    # ğŸ”¥ ìºì‹œ í™•ì¸ (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
    cached_result = cache.get(query, "analyze")
    if cached_result:
        return f"[ìºì‹œë¨] {cached_result}"  # ğŸš€ ì¦‰ì‹œ ë¦¬í„´!
    
    try:
        import re
        import statistics
        from collections import defaultdict
        
        # ë²¡í„° ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        docs = vectorstore.similarity_search(query, k=100)
        
        # ìˆ«ì ë°ì´í„° ì¶”ì¶œ
        data_dict = defaultdict(list)
        
        for doc in docs:
            content = doc.page_content
            
            # ìƒì‚°ëŸ‰, ìˆ˜ì…ëŸ‰, íŒë§¤ëŸ‰ ë“±ì˜ ìˆ«ì ë°ì´í„° ì¶”ì¶œ
            patterns = {
                'ìƒì‚°ëŸ‰': r'ìƒì‚°ëŸ‰:\s*([0-9.]+)',
                'ìˆ˜ì…ëŸ‰': r'ìˆ˜ì…ëŸ‰:\s*([0-9.]+)',
                'íŒë§¤ëŸ‰': r'íŒë§¤ëŸ‰:\s*([0-9.]+)'
            }
            
            for key, pattern in patterns.items():
                matches = re.findall(pattern, content)
                for match in matches:
                    try:
                        value = float(match)
                        if value > 0:  # 0ë³´ë‹¤ í° ê°’ë§Œ í¬í•¨
                            data_dict[key].append(value)
                    except ValueError:
                        continue
        
        # ë¶„ì„ ê²°ê³¼ ìƒì„±
        result = "=== ë°ì´í„° ë¶„ì„ ê²°ê³¼ ===\n"
        
        for key, values in data_dict.items():
            if values:
                result += f"\nã€{key}ã€‘\n"
                result += f"  ë°ì´í„° ê°œìˆ˜: {len(values)}ê°œ\n"
                result += f"  ìµœëŒ€ê°’: {max(values):.2f}\n"
                result += f"  ìµœì†Œê°’: {min(values):.2f}\n"
                result += f"  í‰ê· ê°’: {statistics.mean(values):.2f}\n"
                result += f"  í•©ê³„: {sum(values):.2f}\n"
                
                if len(values) > 1:
                    result += f"  í‘œì¤€í¸ì°¨: {statistics.stdev(values):.2f}\n"
        
        return result if result != "=== ë°ì´í„° ë¶„ì„ ê²°ê³¼ ===\n" else "ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìˆ«ì ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    except Exception as e:
        return f"ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

# tools = [math_tool]
tools = [math_tool, analyze_data]
print(tools[0].name, tools[0].description)

# model(ìƒê°í•  llm, agentì˜ ë‘ë‡Œ),
# tools(agentê°€ ì‚¬ìš©ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡),
# prompt(agentê°€ íŒë‹¨í•  ë•Œ ì–´ë–¤ ê¸°ì¤€ì´ ìˆë‹¤ë©´? ì´ê±¸ í”„ë¡¬í”„íŠ¸ê°€ ë‹´ê³  ìˆë‹¤.)
prompt = ChatPromptTemplate.from_messages([
    ('system', '''ë‹¹ì‹ ì€ í’ˆëª©ë‹¹(ê°ˆì¹˜, ì˜¤ì§•ì–´, ê³ ë“±ì–´) ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ë§Œì•½ í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´ ì €í¬ëŠ” í’ˆëª©ë‹¹ ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ë§Œ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì´ë¼ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì¤˜.
    ëŒ€ë‹µì€ ì¹œì ˆí•˜ê²Œ í•´ì£¼ì„¸ìš”.
    ë‹¤ìŒ ë„êµ¬ë“¤ì„ í™œìš©í•  ìˆ˜ ìˆì–´:
    1. calculator: ìˆ˜í•™ ê³„ì‚°
    2. analyze_data: ë°ì´í„° ë¶„ì„ ë° í†µê³„ ê³„ì‚°
    
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•´ì„œ ì‚¬ìš©í•´ì¤˜.'''),
    ('human', '{input}'),
    ('placeholder', '{agent_scratchpad}')   # toolsì„ ì‚¬ìš©í•˜ë©´ì„œ ë‚¨ê¸°ëŠ” ì¤‘ê°„ ì‘ì—… ë‚´ì—­
])

# agentë¥¼ ë§Œë“¦ => ì´ agentëŠ” toolì„ calling =>
# model(ë‘ë‡Œ)ë¡œ tools(ë„êµ¬)ë¥¼ prompt(ì§€ì‹œì‚¬í•­)ì— ë§ê²Œ íŒë‹¨í•´ì„œ ë¶€ë¦„
agent = create_tool_calling_agent(model, tools, prompt)

# ì—ì´ì „íŠ¸ì˜ ì‹¤í–‰
agent_executor = AgentExecutor(agent=agent, tools=tools)
# result = agent_executor.invoke({'input':'what is 2.1^2.1'})

# --- 6. RetrievalQA ì²´ì¸ êµ¬ì„± ---
qa_chain = RetrievalQA.from_chain_type(
    llm=model,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 30}),
    return_source_documents=True,
)

# --- 7. LangGraph State ì •ì˜ ---
class AgentState(TypedDict):
    query: str
    chat_history: list
    current_time: str
    rag_result: str
    agent_result: str
    final_answer: str
    needs_agent: bool
    source_documents: list

# ==========================================
# í•´ê²°ì±… 1: ì§ˆë¬¸ ì •ê·œí™” ë° ì˜ë¯¸ ê¸°ë°˜ ìºì‹±
# ==========================================

class SmartCache:
    def __init__(self, cache_dir="cache", expire_hours=12):
        self.cache_dir = cache_dir
        self.expire_hours = expire_hours
        self.memory_cache = {}
        self.access_count = 0
        self.last_cleanup = datetime.now()
        self.running = False
        self.cleanup_thread = None
        
        # ğŸ”¥ ì˜ë¯¸ ê¸°ë°˜ ìºì‹±ì„ ìœ„í•œ ì„ë² ë”© ëª¨ë¸
        self.embeddings = embeddings

        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)

        print(f"ğŸ—„ï¸ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì´ˆê¸°í™”: {cache_dir}, ë§Œë£Œ: {expire_hours}ì‹œê°„")
        
        # ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        self._start_background_cleanup()

    def _start_background_cleanup(self):
        """ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘"""
        import threading
        
        self.running = True
        self.cleanup_thread = threading.Thread(target=self._background_worker, daemon=True)
        self.cleanup_thread.start()
        print("ğŸ§µ ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘")
    
    def _background_worker(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ ì •ë¦¬"""
        while self.running:
            try:
                # 30ë¶„ë§ˆë‹¤ ì •ë¦¬
                time.sleep(1800)
                
                if not self.running:
                    break
                
                start_time = time.time()
                cleaned = self._cleanup_expired_caches()
                elapsed = time.time() - start_time
                
                if cleaned > 0:
                    print(f"ğŸ§¹ ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬: {cleaned}ê°œ ì‚­ì œ ({elapsed:.2f}ì´ˆ)")
                
            except Exception as e:
                print(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì˜¤ë¥˜: {e}")
                time.sleep(300)  # ì˜¤ë¥˜ì‹œ 5ë¶„ ëŒ€ê¸°
    
    def _cleanup_expired_caches(self):
        """ë§Œë£Œëœ ìºì‹œë“¤ì„ ì‹¤ì œë¡œ ì •ë¦¬"""
        cleaned_count = 0
        current_time = datetime.now()
        expire_threshold = timedelta(hours=self.expire_hours)
        
        # ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
        expired_keys = []
        for key, cached_data in self.memory_cache.items():
            if current_time - cached_data['timestamp'] > expire_threshold:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.memory_cache[key]
            cleaned_count += 1
        
        # ë””ìŠ¤í¬ ìºì‹œ ì •ë¦¬  
        if os.path.exists(self.cache_dir):
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    file_path = os.path.join(self.cache_dir, filename)
                    try:
                        with open(file_path, 'rb') as f:
                            cached_data = pickle.load(f)
                        
                        if current_time - cached_data['timestamp'] > expire_threshold:
                            os.remove(file_path)
                            cleaned_count += 1
                    except:
                        # ì†ìƒëœ íŒŒì¼ë„ ì‚­ì œ
                        try:
                            os.remove(file_path)
                            cleaned_count += 1
                        except:
                            pass
        
        return cleaned_count
    
    def stop_cleanup(self):
        """ìºì‹œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì¤‘ì§€"""
        self.running = False
        if self.cleanup_thread and self.cleanup_thread.is_alive():
            self.cleanup_thread.join(timeout=5)
        print("ğŸ›‘ ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì •ë¦¬ ì¤‘ì§€")
    
    def __del__(self):
        """ìºì‹œ ê°ì²´ ì†Œë©¸ë¨"""
        self.stop_cleanup()
        
    def _normalize_query(self, query):
        """ì§ˆë¬¸ì„ ì •ê·œí™”í•´ì„œ ìœ ì‚¬í•œ ì§ˆë¬¸ë“¤ì„ ê°™ê²Œ ë§Œë“¦"""        
        # 1. ê¸°ë³¸ ì •ê·œí™”
        normalized = query.lower().strip()
        
        # 2. ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        stop_words = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì„œ', 'ì—ì„œ', 
                     'ì–´ë–»ê²Œ', 'ì–¼ë§ˆ', 'ë­', 'ë¬´ì—‡', 'ì–¸ì œ', 'ì–´ë””ì„œ']
        for word in stop_words:
            normalized = normalized.replace(word, '')
        
        # 3. ê³µë°± ì •ë¦¬
        normalized = ' '.join(normalized.split())
        
        # 4. ë™ì˜ì–´ í†µì¼
        synonyms = {
            'ìƒì‚°ëŸ‰': ['ìƒì‚°', 'ìƒì‚°ëŸ‰', 'ì‚°ì¶œëŸ‰', 'production'],
            'íŒë§¤ëŸ‰': ['íŒë§¤', 'íŒë§¤ëŸ‰', 'ë§¤ì¶œëŸ‰', 'sales'],
            'ìˆ˜ì…ëŸ‰': ['ìˆ˜ì…', 'ìˆ˜ì…ëŸ‰', 'import', 'inbound'],
            'ê°ˆì¹˜': ['ê°ˆì¹˜', 'galchi', 'ê°ˆì¹˜ìƒì„ ', 'cutlassFish'],
            'ì˜¤ì§•ì–´': ['ì˜¤ì§•ì–´', 'squid', 'ì˜¤ì§•ì–´ë¥˜', 'calamari'],
            'ê³ ë“±ì–´': ['ê³ ë“±ì–´', 'mackerel', 'ê³ ë“±ì–´ë¥˜']
        }
        
        for standard, variations in synonyms.items():
            for variation in variations:
                if variation in normalized:
                    normalized = normalized.replace(variation, standard)
        
        return normalized
    
    def _get_semantic_similarity(self, query1, query2):
        """ë‘ ì§ˆë¬¸ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°"""
        try:
            # ì„ë² ë”© ë²¡í„° ìƒì„±
            emb1 = self.embeddings.embed_query(query1)
            emb2 = self.embeddings.embed_query(query2)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            return similarity
        except:
            return 0.0
    
    def _find_similar_cache(self, query, threshold=0.85):
        """ìœ ì‚¬í•œ ì§ˆë¬¸ì˜ ìºì‹œë¥¼ ì°¾ìŒ"""
        normalized_query = self._normalize_query(query)
        
        # 1. ì •ê·œí™”ëœ ì§ˆë¬¸ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ìºì‹œ ì°¾ê¸°
        for cache_key, cached_data in self.memory_cache.items():
            if 'original_query' in cached_data:
                cached_normalized = self._normalize_query(cached_data['original_query'])
                if cached_normalized == normalized_query:
                    return cached_data
        
        # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ ì°¾ê¸°
        best_match = None
        best_similarity = 0
        
        for cache_key, cached_data in self.memory_cache.items():
            if 'original_query' in cached_data:
                similarity = self._get_semantic_similarity(query, cached_data['original_query'])
                if similarity > threshold and similarity > best_similarity:
                    best_similarity = similarity
                    best_match = cached_data
        
        if best_match:
            print(f"ğŸ¯ ì˜ë¯¸ì  ìœ ì‚¬ ìºì‹œ íˆíŠ¸! ìœ ì‚¬ë„: {best_similarity:.2f}")
            return best_match
        
        return None
    
    def _get_cache_key(self, query, context_type="rag"):
        """ìºì‹œ ì¡°íšŒ"""
        cache_key = f"{query}_{context_type}"
        return cache_key
    
    def _is_expired(self, timestamp):
        now = datetime.now()
        hours = now - timestamp
        if hours > timedelta(hours=12):
            print("ìºì‹œ 12ì‹œê°„ ì´ìƒ ì§€ë‚¨")
            return True
        
        return False
    
    def get(self, query, context_type="rag"):
        """ê°œì„ ëœ ìºì‹œ ì¡°íšŒ - ìœ ì‚¬í•œ ì§ˆë¬¸ë„ ì°¾ìŒ"""
        self.access_count += 1
        
        # 1. ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì •í™•í•œ ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(query, context_type)
        if cache_key in self.memory_cache:
            cached_data = self.memory_cache[cache_key]
            if not self._is_expired(cached_data['timestamp']):
                # ğŸ”¥ 100ë²ˆ ì ‘ê·¼ë§ˆë‹¤ ì§€ì—° ì •ë¦¬ ì‹¤í–‰
                if self.access_count % 100 == 0:
                    import threading
                    threading.Thread(target=self._light_cleanup, daemon=True).start()

                print(f"ğŸ’¾ ì •í™•í•œ ìºì‹œ íˆíŠ¸: {query[:30]}...")
                return cached_data
        
        # 2. ìœ ì‚¬í•œ ì§ˆë¬¸ì˜ ìºì‹œ í™•ì¸
        similar_cache = self._find_similar_cache(query)
        if similar_cache and not self._is_expired(similar_cache['timestamp']):
            return similar_cache
        
        # 2. ë””ìŠ¤í¬ ìºì‹œ í™•ì¸ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)
                
                if not self._is_expired(cached_data['timestamp']):
                    self.memory_cache[cache_key] = cached_data
                    print(f"ğŸ’½ ë””ìŠ¤í¬ ìºì‹œ íˆíŠ¸: {query[:30]}...")
                    return cached_data
                else:
                    os.remove(cache_file)
            except:
                pass
        
        return None
    
    def set(self, query, result, context_type="rag"):
        """ìºì‹œ ì €ì¥ ì‹œ ì›ë³¸ ì§ˆë¬¸ë„ í•¨ê»˜ ì €ì¥"""
        cache_key = self._get_cache_key(query, context_type)
        
        cached_data = {
            'result': result,
            'timestamp': datetime.now(),
            'original_query': query,  # ğŸ”¥ ì›ë³¸ ì§ˆë¬¸ ì €ì¥
            'context_type': context_type
        }
        
        self.memory_cache[cache_key] = cached_data
        print(f"ğŸ’¾ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥: {query[:30]}...")
    
    def _light_cleanup(self):
        """ì§€ì—° ì •ë¦¬: ì¼ë¶€ ìºì‹œë§Œ í™•ì¸í•´ì„œ ì •ë¦¬"""
        current_time = datetime.now()
        
        # ìµœê·¼ 5ë¶„ ë‚´ì— ì •ë¦¬í–ˆìœ¼ë©´ ìŠ¤í‚µ
        if current_time - self.last_cleanup < timedelta(minutes=5):
            return
        
        cleaned_count = 0
        checked_count = 0
        
        # ë©”ëª¨ë¦¬ ìºì‹œ ì¼ë¶€ë§Œ í™•ì¸ (ìµœëŒ€ 50ê°œ)
        cache_items = list(self.memory_cache.items())
        for key, cached_data in cache_items[:50]:
            checked_count += 1
            if self._is_expired(cached_data['timestamp']):
                del self.memory_cache[key]
                cleaned_count += 1
        
        if cleaned_count > 0:
            print(f"ğŸŒ ì§€ì—° ì •ë¦¬: {checked_count}ê°œ í™•ì¸, {cleaned_count}ê°œ ì‚­ì œ")
        
        self.last_cleanup = current_time

# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
cache = SmartCache(cache_dir="lang_cache", expire_hours=12)

# --- 8. LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ ---
def classify_query_node(state: AgentState):
    """ì§ˆë¬¸ ë¶„ë¥˜ ë…¸ë“œ - Agentê°€ í•„ìš”í•œì§€ íŒë‹¨"""
    query = state["query"]
    
    # ê³„ì‚°/ë¶„ì„ì´ í•„ìš”í•œ í‚¤ì›Œë“œë“¤
    analysis_keywords = ['ìµœëŒ€', 'ìµœì†Œ', 'í‰ê· ', 'í•©ê³„', 'ë¶„ì„', 'í†µê³„', 'ê³„ì‚°', 'ë¹„êµ', 'ì´í•©', 'ê°€ì¥']
    search_keywords = ['ì°¾ì•„', 'ê²€ìƒ‰', 'ì–¸ì œ', 'ì–´ë–¤', 'ëª‡', 'ì–¼ë§ˆ']
    
    # í‚¤ì›Œë“œì— ë”°ë¼ Agent í•„ìš” ì—¬ë¶€ ê²°ì •
    needs_agent = any(keyword in query for keyword in analysis_keywords + search_keywords)
    
    state["needs_agent"] = needs_agent
    print(f"ğŸ” ì§ˆë¬¸ ë¶„ë¥˜: {'Agent í•„ìš”' if needs_agent else 'RAGë§Œ í•„ìš”'}")
    
    return state

from zoneinfo import ZoneInfo
def get_time(state: AgentState):
    """í˜„ì¬ ë…„ë„ì™€ ì›” ê°€ì ¸ì˜¤ê¸°"""
    KST = ZoneInfo('Asia/Seoul')
    y = datetime.now(KST).year
    m = datetime.now(KST).month

    state["current_time"] = f"{y}ë…„ {m}ì›”"
    # print(f'{state["current_time"]}')

    return state


def rag_node(state: AgentState):
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    print("ğŸ“š RAG ê²€ìƒ‰ ì‹œì‘...")

    # ğŸ”¥ ìºì‹œ í™•ì¸ (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
    cached_result = cache.get(query, "rag")
    if cached_result:
        print("ğŸ’¾ RAG ìºì‹œ íˆíŠ¸!")
        state["rag_result"] = cached_result["result"]
        state["source_documents"] = cached_result.get("source_docs", [])
        return state  # ğŸš€ LLM í˜¸ì¶œ ì—†ì´ ë°”ë¡œ ë¦¬í„´!
    
    try:
        # ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        history_str = "\n".join([f"Human: {q}\nAI: {a}" for q, a in state["chat_history"]])
        
        # RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        rag_prompt = f"""
            ë‹¹ì‹ ì€ í’ˆëª©ë‹¹(ê°ˆì¹˜, ì˜¤ì§•ì–´, ê³ ë“±ì–´) ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ë§Œì•½ í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´ ì €í¬ëŠ” í’ˆëª©ë‹¹ ë‚ ì§œì— ë”°ë¥¸ ìƒì‚°, ìˆ˜ì…, íŒë§¤ëŸ‰ë§Œ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì´ë¼ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì¤˜.
            ëŒ€ë‹µì€ ì¹œì ˆí•˜ê²Œ í•´ì£¼ì„¸ìš”.
            í•„ìš”í•˜ë‹¤ë©´ ë‹¤ìŒì˜ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
            ë§Œì•½ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.

            í˜„ì¬ ì‹œê°: {state["current_time"]}

            ëŒ€í™” ê¸°ë¡:
            {history_str}

            ì§ˆë¬¸: {state["query"]}
        """
        
        response = qa_chain.invoke({"query": rag_prompt})
        state["rag_result"] = response['result']
        state["source_documents"] = response.get('source_documents', [])
        
        print(f"âœ… RAG ê²€ìƒ‰ ì™„ë£Œ: {len(state['source_documents'])}ê°œ ë¬¸ì„œ ì°¸ì¡°")

        cache.set(query=query, result=response['result'], context_type="rag")
        
    except Exception as e:
        print(f"âŒ RAG ì˜¤ë¥˜: {e}")
        state["rag_result"] = f"RAG ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        state["source_documents"] = []
    
    return state

def agent_node(state: AgentState):
    """Agent ë¶„ì„ ë…¸ë“œ"""
    print("ğŸ¤– Agent ë¶„ì„ ì‹œì‘...")

    # ğŸ”¥ ë³µí•© ìºì‹œ í‚¤ ìƒì„± (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
    cached_result = cache.get(query, "agent")

    if cached_result:
        print("ğŸ’¾ Agent ìºì‹œ íˆíŠ¸!")
        state["agent_result"] = cached_result["result"]
        return state  # ğŸš€ Agent ì‹¤í–‰ ì—†ì´ ë°”ë¡œ ë¦¬í„´!
    
    try:
        # RAG ê²°ê³¼ë¥¼ í¬í•¨í•œ ì…ë ¥ êµ¬ì„±
        # RAG ê²€ìƒ‰ ê²°ê³¼: {state["rag_result"]}
        # RAG ê²€ìƒ‰ ê²°ê³¼: {texts}

        # ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        history_str = "\n".join([f"Human: {q}\nAI: {a}" for q, a in state["chat_history"]])

        agent_input = f"""
        í˜„ì¬ ì‹œê°: {state["current_time"]}

        ê²€ìƒ‰ ê²°ê³¼: {texts}
        
        ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸: {query}

        ëŒ€í™” ê¸°ë¡:
        {history_str}
        
        ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•„ìš”í•œ ë¶„ì„ì´ë‚˜ ê³„ì‚°ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ì¶”ê°€ì ì¸ ë°ì´í„° ë¶„ì„ì´ í•„ìš”í•˜ë‹¤ë©´ analyze_data ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê³ ,
        ìˆ˜í•™ ê³„ì‚°ì´ í•„ìš”í•˜ë‹¤ë©´ calculator ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
        """
        
        response = agent_executor.invoke({"input": agent_input})
        state["agent_result"] = response['output']
        
        print("âœ… Agent ë¶„ì„ ì™„ë£Œ")

        cache.set(query=query, result=response['output'], context_type="agent")
        
    except Exception as e:
        print(f"âŒ Agent ì˜¤ë¥˜: {e}")
        state["agent_result"] = f"Agent ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
    
    return state

def combine_results_node(state: AgentState):
    """ê²°ê³¼ í†µí•© ë…¸ë“œ"""
    print("ğŸ”— ê²°ê³¼ í†µí•© ì¤‘...")
    
    if state["needs_agent"] and state["agent_result"] and "ì˜¤ë¥˜" not in state["agent_result"]:
        # Agent ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        state["final_answer"] = f"""{state["agent_result"]}

{state["rag_result"]}"""
    else:
        # RAGë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        state["final_answer"] = state["rag_result"]
    
    print("âœ… ê²°ê³¼ í†µí•© ì™„ë£Œ")
    return state

# --- 9. ë¼ìš°íŒ… í•¨ìˆ˜ ---
def decide_next_step(state: AgentState):
    """ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°íŒ… í•¨ìˆ˜"""
    if state["needs_agent"]:
        return "agent"
    else:
        return "rag"

# --- 10. LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ---
workflow = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("classify", classify_query_node)
workflow.add_node("time", get_time)
workflow.add_node("rag", rag_node)
workflow.add_node("agent", agent_node)
workflow.add_node("combine", combine_results_node)

# ì—£ì§€ ì¶”ê°€
def workflow_add_edge_2():
    workflow.add_edge(START, "classify")
    workflow.add_edge("classify", "time")
    workflow.add_conditional_edges(
        "time",
        decide_next_step,
        {
            "agent": "agent",
            "rag": "rag"
        }
    )
    workflow.add_edge("agent", "combine")
    workflow.add_edge("rag", "combine")
    workflow.add_edge("combine", END)

workflow_add_edge_2()

# ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼
app = workflow.compile()
def lang_graph_png():
    from IPython.display import Image, display

    try:
        #ìš°ë¦¬ê°€ ë§Œë“  appì˜ graphë¥¼ ì´ë¯¸ì§€ë¡œ ê·¸ë ¤ì„œ display()ë¡œ ê°ì‹¸ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
        with open("LangGraph.png", "wb") as f:
            f.write(app.get_graph().draw_mermaid_png())
        # display(Image("LangGraph.png"))

    except Exception:
        print("ë­ê·¸ë˜í”„ png ì‹¤íŒ¨")

# lang_graph_png()

print("ğŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ì¤€ë¹„ ì™„ë£Œ!")

# --- 11. ë©”ì¸ ëŒ€í™” ë£¨í”„ ---
chat_history = []

while True:
    query = input("\nì§ˆë¬¸í•´ì£¼ì„¸ìš”. >> ")
    if query.lower() == "exit":
        break
    
    print("\n" + "="*60)
    print(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {query}")
    print("="*60)
    
    # LangGraph ì‹¤í–‰
    try:
        result = app.invoke({
            "query": query,
            "chat_history": chat_history,
            "current_time": "",
            "rag_result": "",
            "agent_result": "",
            "final_answer": "",
            "needs_agent": False,
            "source_documents": []
        })
        
        # ê²°ê³¼ ì¶œë ¥
        final_answer = result['final_answer']
        source_docs = result['source_documents']
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ (ìµœëŒ€ 3ê°œ ìœ ì§€)
        chat_history.append((query, final_answer))
        if len(chat_history) > 3:
            chat_history.pop(0)
        
        # ê²°ê³¼ í‘œì‹œ
        print("\n" + "-"*50)
        if source_docs:
            print(f"ğŸ“Š ì°¸ì¡°ëœ ë¬¸ì„œ ìˆ˜: {len(source_docs)}ê°œ")
        print(f"ğŸ’¬ ë‹µë³€:\n{final_answer}")
        print("="*50)

        # if cache.get(query, "rag"):
        #     print(f"ğŸ’¾ ìºì‹œ í™œìš©ìœ¼ë¡œ í† í° ì ˆì•½!")  # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€
        # else:
        #     print(f"ìºì‹œ ì¶”ê°€!!!")
        
    except Exception as e:
        print(f"âŒ ì „ì²´ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        print("="*50)

# ê°ˆì¹˜ì˜ í‰ê·  ìƒì‚°ëŸ‰
# ì´ 9693
# í‰ê·  1,384.714285

# ì˜¤ì§•ì–´ì˜ í‰ê·  ìƒì‚°ëŸ‰
# ì´ 6435
# í‰ê·  919.285714

# ê³ ë“±ì–´ì˜ í‰ê·  ìƒì‚°ëŸ‰
# ì´ 60,483
# í‰ê·  8,640.4285